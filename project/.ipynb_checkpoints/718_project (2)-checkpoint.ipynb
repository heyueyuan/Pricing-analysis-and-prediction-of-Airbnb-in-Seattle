{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "zFm3us3bspzY",
    "outputId": "9787997f-9bd3-4d7b-bf34-3a3b164a0d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from folium) (1.12.0)\n",
      "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from folium) (2.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from folium) (1.18.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "#@ install Spark and dependency\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3AM2cElyLOf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8_RrCFJzgaL"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5b3SChpYwcEJ"
   },
   "source": [
    "# Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "EqubFjdBtMcG",
    "outputId": "9d6064dc-7671-4e9f-9f7a-1efc68903b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSaffUGCwqBE"
   },
   "source": [
    "Dataset Path\n",
    "\n",
    "---\n",
    "\n",
    "/content/drive/Shared drives/718_project/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c8nTrg1JtM7S",
    "outputId": "3d779bf4-8265-4c57-d39d-338dc0767582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/Shared drives/718_project/dataset\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/Shared drives/718_project/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "L7Zv96FftUda",
    "outputId": "16c3ab35-3af1-4430-b9e5-85e4f2e0bda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar.csv       listings.csv  \u001b[0m\u001b[01;34mspark-2.4.4-bin-hadoop2.7\u001b[0m/\n",
      "listing_clean.csv  reviews.csv   spark-2.4.4-bin-hadoop2.7.tgz\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXxwvKCew775"
   },
   "source": [
    "Data Path\n",
    "\n",
    "---\n",
    "/content/drive/Shared drives/718_project/dataset/listings.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcZmk0xfxLah"
   },
   "source": [
    "Load Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaY1mvHzxCT2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from  pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPF7YNEcndbX"
   },
   "source": [
    "Create data repository decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5_XK8zw0zem"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from collections.abc import Iterable\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "def train_test_dataset_repository(path, target, features=None):\n",
    "  param_features = features\n",
    "  if target is None:\n",
    "    raise AttributeError(\"Parameter 'target' missing! \")\n",
    "  def build_dataset(func):\n",
    "    def wrapper_func(self):\n",
    "      # Raw data\n",
    "      raw_data = spark.read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"multiLine\", \"true\")\\\n",
    "        .option('inferSchema', 'true')\\\n",
    "        .option('escape', '\"')\\\n",
    "        .csv(path)\n",
    "      columns = raw_data.schema.names\n",
    "\n",
    "      # Select features\n",
    "      features = param_features\n",
    "      if features is None:\n",
    "        features = columns\n",
    "      if not isinstance(features, Iterable):\n",
    "        features = [features]\n",
    "      features.remove(target)\n",
    "\n",
    "      # Recording raw_data\n",
    "      self._raw_data = raw_data\n",
    "      # Recording configuration\n",
    "      self._config = dict({\n",
    "          'path': path,\n",
    "          'features': features,\n",
    "          'target': target,\n",
    "      })\n",
    "\n",
    "      # Build X, Y\n",
    "      return reset_dataset_from_raw(func)(self)\n",
    "    return wrapper_func\n",
    "  return build_dataset\n",
    "\n",
    "def reset_dataset_from_raw(func):\n",
    "  def wrapper_func(self):\n",
    "    raw_data = self._raw_data\n",
    "    # Restore config\n",
    "    features = self._config['features']\n",
    "    target = self._config['target']\n",
    "\n",
    "    \n",
    "    X = raw_data.select([fn.col(col_name) for col_name in features])\n",
    "    Y = raw_data.select(fn.col(target))\n",
    "    self._X = X\n",
    "    self._Y = Y\n",
    "\n",
    "    train_data, test_data = raw_data.randomSplit([0.7, 0.3])\n",
    "    self._train_X = train_data.select([fn.col(col_name) for col_name in features])\n",
    "    self._train_Y = train_data.select(fn.col(target))\n",
    "    self._test_X = test_data.select([fn.col(col_name) for col_name in features])\n",
    "    self._test_Y = test_data.select(fn.col(target))\n",
    "\n",
    "    return func(self)\n",
    "  return wrapper_func\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2ApdKLXnoed"
   },
   "source": [
    "Create data object, fill the raw/train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QDlxabenti8"
   },
   "outputs": [],
   "source": [
    "def warning(msg):\n",
    "  flag = True\n",
    "  def inject(func):\n",
    "    def wrapper_func(self):\n",
    "      x = 'Y'\n",
    "      if flag:\n",
    "        x = input(f\"{func.__name__}是{msg}，确定要用？(Y/N)\")\n",
    "      if 'Y' == x:\n",
    "        return func(self)\n",
    "      raise PermissionError(f\"{func.__name__}是{msg}，不能随便用！\")\n",
    "    return wrapper_func\n",
    "  return inject\n",
    "\n",
    "class Data:\n",
    "  @train_test_dataset_repository('/content/drive/Shared drives/718_project/dataset/listing_clean.csv', 'price')\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  @property\n",
    "  @warning(\"原始未分割数据\")\n",
    "  def raw_data(self):\n",
    "    return self._raw_data\n",
    "  @property\n",
    "  @warning(\"原始自变量\")\n",
    "  def X(self):\n",
    "    return self._X\n",
    "  @property\n",
    "  @warning(\"原始应变量\")\n",
    "  def Y(self):\n",
    "    return self._Y\n",
    "  @property\n",
    "  def train_X(self):\n",
    "    return self._train_X\n",
    "  @property\n",
    "  def train_Y(self):\n",
    "    return self._train_Y\n",
    "  @property\n",
    "  @warning(\"测试自变量\")\n",
    "  def test_X(self):\n",
    "    return self._test_X\n",
    "  @property\n",
    "  @warning(\"测试应变量\")\n",
    "  def test_Y(self):\n",
    "    return self._test_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkTfWj7V_5rQ"
   },
   "source": [
    "# EDA and Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6zcGsLil9My"
   },
   "source": [
    "We will explore the whole dataset during the EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "icMTaZsEl9y1",
    "outputId": "8df69cd4-a184-4b7f-bf5a-88aa447ec4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data是原始未分割数据，确定要用？(Y/N)Y\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "df = Data().raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "L3GtZTZZP1pO",
    "outputId": "2a14a703-e77a-495e-ace6-0bfa530193e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7628 entries, 0 to 7627\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   price   7628 non-null   float32\n",
      " 1   price   7628 non-null   float32\n",
      "dtypes: float32(2)\n",
      "memory usage: 59.7 KB\n"
     ]
    }
   ],
   "source": [
    "# 数据信息查看\n",
    "df_pd = df.toPandas()\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hlwJ8OH4XVR"
   },
   "source": [
    "## Numerical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2AzLRdl5lmT"
   },
   "source": [
    "### Convert values in columns from string to number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwoEvaeM5yoC"
   },
   "source": [
    "- Display the type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vAfLBuhD4e5b",
    "outputId": "3d884892-c8c2-4ee8-fa1f-86dd87c29d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- experiences_offered: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: timestamp (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: double (nullable = true)\n",
      " |-- host_is_superhost: integer (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_identity_verified: integer (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- smart_location: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- is_location_exact: integer (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- guests_included: integer (nullable = true)\n",
      " |-- extra_people: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: timestamp (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- first_review: timestamp (nullable = true)\n",
      " |-- last_review: timestamp (nullable = true)\n",
      " |-- review_scores_rating: integer (nullable = true)\n",
      " |-- review_scores_accuracy: integer (nullable = true)\n",
      " |-- review_scores_cleanliness: integer (nullable = true)\n",
      " |-- review_scores_checkin: integer (nullable = true)\n",
      " |-- review_scores_communication: integer (nullable = true)\n",
      " |-- review_scores_location: integer (nullable = true)\n",
      " |-- review_scores_value: integer (nullable = true)\n",
      " |-- jurisdiction_names: string (nullable = true)\n",
      " |-- instant_bookable: integer (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- require_guest_profile_picture: integer (nullable = true)\n",
      " |-- require_guest_phone_verification: integer (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hORW6ywl6LSz"
   },
   "source": [
    "- Select boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CA6SxxOz6NbX",
    "outputId": "085bcf5b-8514-46c1-b2b4-875d90a29b3d"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/content/drive/Shared drives/718_project/dataset/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`host_has_profile_pic`' given input columns: [review_scores_location, maximum_nights, extra_people, is_location_exact, host_url, review_scores_checkin, number_of_reviews, neighbourhood_group_cleansed, name, calendar_updated, host_total_listings_count, _c0, minimum_minimum_nights, maximum_maximum_nights, city, bed_type, country, minimum_maximum_nights, neighbourhood_cleansed, id, picture_url, availability_365, street, host_response_time, host_response_rate, maximum_minimum_nights, country_code, amenities, calculated_host_listings_count_shared_rooms, bathrooms, minimum_nights, host_since, smart_location, description, host_location, host_listings_count, host_identity_verified, host_thumbnail_url, cancellation_policy, review_scores_value, host_id, require_guest_profile_picture, host_is_superhost, calculated_host_listings_count_private_rooms, state, minimum_nights_avg_ntm, review_scores_cleanliness, reviews_per_month, guests_included, beds, maximum_nights_avg_ntm, host_picture_url, property_type, availability_60, room_type, longitude, price, number_of_reviews_ltm, availability_90, accommodates, availability_30, require_guest_phone_verification, calculated_host_listings_count_entire_homes, host_verifications, experiences_offered, review_scores_accuracy, listing_url, neighbourhood, review_scores_communication, bedrooms, jurisdiction_names, instant_bookable, review_scores_rating, latitude, first_review, market, host_name, c...\n'Project [host_is_superhost#14, 'host_has_profile_pic, host_identity_verified#20, is_location_exact#33, 'has_availability, 'requires_license, instant_bookable#71, 'is_business_travel_ready, require_guest_profile_picture#73, require_guest_phone_verification#74]\n+- Relation[_c0#0,id#1,listing_url#2,name#3,description#4,experiences_offered#5,picture_url#6,host_id#7,host_url#8,host_name#9,host_since#10,host_location#11,host_response_time#12,host_response_rate#13,host_is_superhost#14,host_thumbnail_url#15,host_picture_url#16,host_listings_count#17,host_total_listings_count#18,host_verifications#19,host_identity_verified#20,street#21,neighbourhood#22,neighbourhood_cleansed#23,... 56 more fields] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8243cd0a2d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;34m'require_guest_profile_picture'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     'require_guest_phone_verification']\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdf_bool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/Shared drives/718_project/dataset/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \"\"\"\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/Shared drives/718_project/dataset/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`host_has_profile_pic`' given input columns: [review_scores_location, maximum_nights, extra_people, is_location_exact, host_url, review_scores_checkin, number_of_reviews, neighbourhood_group_cleansed, name, calendar_updated, host_total_listings_count, _c0, minimum_minimum_nights, maximum_maximum_nights, city, bed_type, country, minimum_maximum_nights, neighbourhood_cleansed, id, picture_url, availability_365, street, host_response_time, host_response_rate, maximum_minimum_nights, country_code, amenities, calculated_host_listings_count_shared_rooms, bathrooms, minimum_nights, host_since, smart_location, description, host_location, host_listings_count, host_identity_verified, host_thumbnail_url, cancellation_policy, review_scores_value, host_id, require_guest_profile_picture, host_is_superhost, calculated_host_listings_count_private_rooms, state, minimum_nights_avg_ntm, review_scores_cleanliness, reviews_per_month, guests_included, beds, maximum_nights_avg_ntm, host_picture_url, property_type, availability_60, room_type, longitude, price, number_of_reviews_ltm, availability_90, accommodates, availability_30, require_guest_phone_verification, calculated_host_listings_count_entire_homes, host_verifications, experiences_offered, review_scores_accuracy, listing_url, neighbourhood, review_scores_communication, bedrooms, jurisdiction_names, instant_bookable, review_scores_rating, latitude, first_review, market, host_name, calendar_las..."
     ]
    }
   ],
   "source": [
    "bool_columns = ['host_is_superhost', \n",
    "                    'host_has_profile_pic', \n",
    "                    'host_identity_verified', \n",
    "                    'is_location_exact', \n",
    "                    'has_availability', \n",
    "                    'requires_license', \n",
    "                    'instant_bookable', \n",
    "                    'is_business_travel_ready', \n",
    "                    'require_guest_profile_picture', \n",
    "                    'require_guest_phone_verification']\n",
    "df_bool = df.select(bool_columns)\n",
    "df_bool.limit(5).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlWlQqqRB09H"
   },
   "source": [
    "- Replace t to 1, and f to 0 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zWvZm7saB5-F",
    "outputId": "7109aa60-72f1-4038-a4e7-5bee3661c318"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/content/drive/Shared drives/718_project/dataset/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o386.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`host_has_profile_pic`' given input columns: [review_scores_location, maximum_nights, extra_people, is_location_exact, host_url, review_scores_checkin, number_of_reviews, neighbourhood_group_cleansed, name, calendar_updated, host_total_listings_count, _c0, minimum_minimum_nights, maximum_maximum_nights, city, bed_type, country, minimum_maximum_nights, neighbourhood_cleansed, id, picture_url, availability_365, street, host_response_time, host_response_rate, maximum_minimum_nights, country_code, amenities, calculated_host_listings_count_shared_rooms, bathrooms, minimum_nights, host_since, smart_location, description, host_location, host_listings_count, host_identity_verified, host_thumbnail_url, cancellation_policy, review_scores_value, host_id, require_guest_profile_picture, calculated_host_listings_count_private_rooms, state, minimum_nights_avg_ntm, review_scores_cleanliness, reviews_per_month, guests_included, beds, maximum_nights_avg_ntm, host_picture_url, property_type, availability_60, room_type, longitude, price, number_of_reviews_ltm, availability_90, accommodates, availability_30, require_guest_phone_verification, calculated_host_listings_count_entire_homes, host_verifications, experiences_offered, review_scores_accuracy, listing_url, neighbourhood, review_scores_communication, bedrooms, jurisdiction_names, host_is_superhost, instant_bookable, review_scores_rating, latitude, first_review, market, host_name, c...\n'Project [_c0#0, id#1, listing_url#2, name#3, description#4, experiences_offered#5, picture_url#6, host_id#7, host_url#8, host_name#9, host_since#10, host_location#11, host_response_time#12, host_response_rate#13, host_is_superhost#481, host_thumbnail_url#15, host_picture_url#16, host_listings_count#17, host_total_listings_count#18, host_verifications#19, host_identity_verified#20, street#21, neighbourhood#22, neighbourhood_cleansed#23, ... 57 more fields]\n+- Project [_c0#0, id#1, listing_url#2, name#3, description#4, experiences_offered#5, picture_url#6, host_id#7, host_url#8, host_name#9, host_since#10, host_location#11, host_response_time#12, host_response_rate#13, bool_map(host_is_superhost#14) AS host_is_superhost#481, host_thumbnail_url#15, host_picture_url#16, host_listings_count#17, host_total_listings_count#18, host_verifications#19, host_identity_verified#20, street#21, neighbourhood#22, neighbourhood_cleansed#23, ... 56 more fields]\n   +- Relation[_c0#0,id#1,listing_url#2,name#3,description#4,experiences_offered#5,picture_url#6,host_id#7,host_url#8,host_name#9,host_since#10,host_location#11,host_response_time#12,host_response_rate#13,host_is_superhost#14,host_thumbnail_url#15,host_picture_url#16,host_listings_count#17,host_total_listings_count#18,host_verifications#19,host_identity_verified#20,street#21,neighbourhood#22,neighbourhood_cleansed#23,... 56 more fields] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:357)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:357)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-67e3588fdebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbool_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_encode_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbool_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/Shared drives/718_project/dataset/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \"\"\"\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/Shared drives/718_project/dataset/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`host_has_profile_pic`' given input columns: [review_scores_location, maximum_nights, extra_people, is_location_exact, host_url, review_scores_checkin, number_of_reviews, neighbourhood_group_cleansed, name, calendar_updated, host_total_listings_count, _c0, minimum_minimum_nights, maximum_maximum_nights, city, bed_type, country, minimum_maximum_nights, neighbourhood_cleansed, id, picture_url, availability_365, street, host_response_time, host_response_rate, maximum_minimum_nights, country_code, amenities, calculated_host_listings_count_shared_rooms, bathrooms, minimum_nights, host_since, smart_location, description, host_location, host_listings_count, host_identity_verified, host_thumbnail_url, cancellation_policy, review_scores_value, host_id, require_guest_profile_picture, calculated_host_listings_count_private_rooms, state, minimum_nights_avg_ntm, review_scores_cleanliness, reviews_per_month, guests_included, beds, maximum_nights_avg_ntm, host_picture_url, property_type, availability_60, room_type, longitude, price, number_of_reviews_ltm, availability_90, accommodates, availability_30, require_guest_phone_verification, calculated_host_listings_count_entire_homes, host_verifications, experiences_offered, review_scores_accuracy, listing_url, neighbourhood, review_scores_communication, bedrooms, jurisdiction_names, host_is_superhost, instant_bookable, review_scores_rating, latitude, first_review, market, host_name, calendar_las..."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "bool_dict = {'t': 1, 'f': 0}\n",
    "def bool_map(x):\n",
    "  if x in bool_dict.keys():\n",
    "    return bool_dict[x]\n",
    "  return x\n",
    "\n",
    "bool_encode_udf = fn.udf(bool_map, t.IntegerType())\n",
    "\n",
    "for col_name in bool_columns:\n",
    "  df = df.withColumn(col_name, bool_encode_udf(fn.col(col_name)))\n",
    "\n",
    "df.select([fn.col(col_name) for col_name in bool_columns]).limit(5).toPandas().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNA7pgs2O_gF"
   },
   "source": [
    "### Convert values in columns from formatted string to number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUm23gfFFgDu"
   },
   "source": [
    "- Select price formatted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "FMfX7GY8Petn",
    "outputId": "f29efab0-8636-40ef-fcc5-6b4676bd1716"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>extra_people</th>\n",
       "      <td>$25.00</td>\n",
       "      <td>$5.00</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>$15.00</td>\n",
       "      <td>$15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>$296.00</td>\n",
       "      <td>$48.00</td>\n",
       "      <td>$90.00</td>\n",
       "      <td>$62.00</td>\n",
       "      <td>$99.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0       1       2       3       4\n",
       "extra_people   $25.00   $5.00  $10.00  $15.00  $15.00\n",
       "price         $296.00  $48.00  $90.00  $62.00  $99.00"
      ]
     },
     "execution_count": 155,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_columns = ['extra_people', \n",
    "                 'price']\n",
    "df_price = df.select(price_columns)\n",
    "df_price.limit(5).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfgT5TAZRkDr"
   },
   "source": [
    " - Reformat the currency formattet to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "fNEtgXuSRtT8",
    "outputId": "0787b3b6-7644-457e-d320-fbd96020b59d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>extra_people</th>\n",
       "      <td>25.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>296.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4\n",
       "extra_people   25.00   5.00  10.00  15.00  15.00\n",
       "price         296.00  48.00  90.00  62.00  99.00"
      ]
     },
     "execution_count": 156,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col_name in price_columns:\n",
    "  df = df.withColumn(col_name, fn.regexp_replace(fn.col(col_name), \"\\$|,\" , '' ))\n",
    "\n",
    "df.select([fn.col(col_name) for col_name in price_columns]).limit(5).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQvuJqymn9gS"
   },
   "outputs": [],
   "source": [
    "#Cth's code 将price 从string type 转变为 numeric type\n",
    "df = df.withColumn('price', df.price.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WMnHzfiDoNJI",
    "outputId": "290ad621-216e-4b0f-9297-68876e837790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- experiences_offered: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: timestamp (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: double (nullable = true)\n",
      " |-- host_is_superhost: integer (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_identity_verified: integer (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- smart_location: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- is_location_exact: integer (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- guests_included: integer (nullable = true)\n",
      " |-- extra_people: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: timestamp (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- first_review: timestamp (nullable = true)\n",
      " |-- last_review: timestamp (nullable = true)\n",
      " |-- review_scores_rating: integer (nullable = true)\n",
      " |-- review_scores_accuracy: integer (nullable = true)\n",
      " |-- review_scores_cleanliness: integer (nullable = true)\n",
      " |-- review_scores_checkin: integer (nullable = true)\n",
      " |-- review_scores_communication: integer (nullable = true)\n",
      " |-- review_scores_location: integer (nullable = true)\n",
      " |-- review_scores_value: integer (nullable = true)\n",
      " |-- jurisdiction_names: string (nullable = true)\n",
      " |-- instant_bookable: integer (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- require_guest_profile_picture: integer (nullable = true)\n",
      " |-- require_guest_phone_verification: integer (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-_7M5p0nPgM"
   },
   "source": [
    "## Dummy Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFyPD-WpnPOs"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dr-aMwv6nLEN"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4OV8u-A-P7P"
   },
   "source": [
    "## Zero Variance or low Variance Variables Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2r72UtqOzyve"
   },
   "outputs": [],
   "source": [
    "#CTH's code\n",
    "drop_list_low_variance = []\n",
    "def low_variance(col,df):\n",
    "  var = df.agg({col : 'variance'})\n",
    "  if var.collect()[0][0] < 0.005:\n",
    "    drop_list_low_variance.append(col)\n",
    "    return var.show()\n",
    "for i in bool_columns:\n",
    "    low_variance(i,df)\n",
    "drop_list_low_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55QAsBIQ-TbI"
   },
   "outputs": [],
   "source": [
    "#var = df.agg({'is_business_travel_ready': 'variance'}).show()\n",
    "#df.drop('is_business_travel_ready')\n",
    "\n",
    "#df.agg({'host_has_profile_pic' : 'variance'}).show()\n",
    "#df.drop('host_has_profile_pic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hbic-ZJ2jhT"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_2LECKs2j7b"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1bMFXVmmA_v"
   },
   "source": [
    "## Missing value exploratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_6dWZ3LmBR-"
   },
   "source": [
    "- Identify the number of missing value at each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I84RLe3hmEiL"
   },
   "outputs": [],
   "source": [
    "def calc_missing_ratio(data_frame: DataFrame):\n",
    "  record_cnt = data_frame.count()\n",
    "  df_columns = data_frame.columns\n",
    "  \n",
    "  df_result = data_frame.select([fn.col(c).cast(t.StringType()) for c in data_frame.columns]) \\\n",
    "    .select([fn.sum(fn.when(fn.isnull(c), 1).otherwise(0)).alias(c) for c in data_frame.columns]) \\\n",
    "    .select([(col(c)/record_cnt).alias(c) for c in data_frame.columns]) \\\n",
    "    .toPandas().T\n",
    "\n",
    "  df_result = df_result.loc[(df_result != 0).all(axis=1), :]\n",
    "  df_result.columns = ['Missing Value Ratio']\n",
    "  df_result.sort_values(by=['Missing Value Ratio'], ascending=False, inplace=True)\n",
    "\n",
    "  return df_result\n",
    "\n",
    "missing_ratio = calc_missing_ratio(df)\n",
    "\n",
    "print(missing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rCfDDb3OmGKK"
   },
   "source": [
    "- Drop columns that doesn't have value at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kzz86OLUmIRU"
   },
   "outputs": [],
   "source": [
    "def shape(data_frame: DataFrame):\n",
    "  return (data_frame.count(), len(data_frame.columns))\n",
    "\n",
    "empty_column = list(missing_ratio.loc[(missing_ratio == 1).all(axis=1), :].index)\n",
    "\n",
    "df = df.drop(*empty_column)\n",
    "shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ge9TujRVmL9E"
   },
   "source": [
    "- Visualize the ratio of missing values using bar plot.<br>\n",
    "We will focus on the columns that has more than 2% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8nj1AQqpmO-d"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "def plot_missing_freq(df, xlab, ylab, title):\n",
    "  plt.clf()\n",
    "  plt.figure(figsize=(20, 5))\n",
    "  df = pd.Series(df.iloc[:, 0].values, index=list(df.index))\n",
    "\n",
    "  ax = sns.barplot(x=df.values * 100, y=df.index, orient='h')\n",
    "  \n",
    "  ax.set_xticklabels(df.values * 100, fontsize=15)\n",
    "  ax.set_yticklabels(df.index, fontsize=12)\n",
    "  ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "  ax.set_xlabel(xlab, fontsize=15)\n",
    "  ax.set_ylabel(ylab, fontsize=35)\n",
    "\n",
    "  plt.title(title, fontsize=20)\n",
    "  # for bar in ax.patches:\n",
    "  #   bar.set_height(30)\n",
    "\n",
    "  display()\n",
    "\n",
    "missing_ratio_2 = calc_missing_ratio(df)\n",
    "missing_ratio_2 = missing_ratio_2.loc[(missing_ratio_2 > 0.02).all(axis=1), :]\n",
    "plot_missing_freq(missing_ratio_2, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHhZ-cR1ot-L"
   },
   "outputs": [],
   "source": [
    "#Mark\n",
    "pred_list = ['id','host_is_superhost','neighbourhood_group_cleansed', 'property_type', \n",
    "             'room_type', 'latitude', 'longitude', 'guests_included', 'bathrooms', \n",
    "             'bedrooms', 'beds', 'bed_type', 'amenities', 'price', 'cleaning_fee', \n",
    "             'instant_bookable', 'cancellation_policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b9pe_6nPQcD"
   },
   "outputs": [],
   "source": [
    "df.select(pred_list).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adMtwWUTQIIQ"
   },
   "outputs": [],
   "source": [
    "pred_df = df.select(pred_list)\n",
    "pred_df  = pred_df.na.drop()\n",
    "pred_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hyw5Ayh0ISlw"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2_yQId5nfyn"
   },
   "outputs": [],
   "source": [
    "#Dummy Variable\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "dummy_list = ['neighbourhood_group_cleansed',\n",
    "       'property_type', 'room_type', 'bed_type','cancellation_policy']\n",
    "\n",
    "def dummy_convert(dummy_list,df):\n",
    "    for i in dummy_list:\n",
    "        categ = df.select(i).distinct().rdd.flatMap(lambda x:x).collect()\n",
    "        exprs = [F.when(F.col(i) == j,1).otherwise(0).alias(str(j)) for j in categ]\n",
    "        df = df.select(exprs+df.columns)\n",
    "    return df\n",
    "\n",
    "dummy_convert(dummy_list,pred_df).toPandas()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwuYS9DqQIcA"
   },
   "outputs": [],
   "source": [
    "#CTH's code 将一些字符串形式的数字中的N/A 用平均数代替 \n",
    "df = df.withColumn('host_response_rate', regexp_replace('host_response_rate', '%', ''))\n",
    "df = df.withColumn('host_response_rate', df['host_response_rate'].cast(\"double\"))\n",
    "mean = df.agg({'host_response_rate': 'mean'}).collect()[0][0]\n",
    "df = df.fillna({'host_response_rate': mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2-nsfUJ_dFh"
   },
   "outputs": [],
   "source": [
    "# CTH's code\n",
    "# drop_list1是一些所有行或者大部分行都缺失的列和一些无用的列\n",
    "drop_list1 = ['license', 'scrape_id','last_scraped',\n",
    "                ,'host_acceptance_rate','thumbnail_url','medium_url','xl_picture_url'\n",
    "                ,'square_feet','weekly_price','monthly_price'\n",
    "                ,'host_neighbourhood','zipcode','security_deposit'\n",
    "                , 'cleaning_fee','summary',]\n",
    "\n",
    "# drop_list2中的列有大量缺失值,都是文字性信息\n",
    "drop_list2 = ['notes','access','transit','space','neighborhood_overview','interaction','house_rules','host_about']\n",
    "\n",
    "# drop_list_low_variance = ['host_has_profile_pic','has_availability','requires_license','is_business_travel_ready']\n",
    "# low variance 关于boolean的四个要删除的columns\n",
    "\n",
    "df = df.drop(*drop_list1)\n",
    "df = df.drop(*drop_list2)\n",
    "df = df.drop(*drop_list_low_variance)\n",
    "print(df.count(),len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyRWB64U_gEk"
   },
   "outputs": [],
   "source": [
    "# CTH's code\n",
    "#查看drop部分columns以后NA的情况\n",
    "i = 0\n",
    "j = 10\n",
    "for k in range(9):\n",
    "    df.select([count(when(isnull(c), c)).alias(c) for c in df.columns[i:j]]).show()\n",
    "    i +=10\n",
    "    j +=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31V-Sei0_oxC"
   },
   "outputs": [],
   "source": [
    "# CTH's code\n",
    "# subset中的列缺失值大于600个， drop掉这12列中缺失值数量大于等于7个的列（也就是超过一半都是NA的行）\n",
    "df = df.dropna(subset=[\"first_review\",\"last_review\",\"review_scores_rating\",\"review_scores_accuracy\"\n",
    "                    ,\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\"\n",
    "                   ,\"review_scores_location\",\"review_scores_value\"\n",
    "                    ],thresh=5)\n",
    "print(df.count(),len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seBgA3Jc_yfm"
   },
   "outputs": [],
   "source": [
    "# CTH's code\n",
    "#查看第二次drop后NA的情况\n",
    "i = 0\n",
    "j = 10\n",
    "for k in range(8):\n",
    "    df.select([count(when(isnull(c), c)).alias(c) for c in df.columns[i:j]]).show()\n",
    "    i +=10\n",
    "    j +=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zo7NUE4AChZ"
   },
   "outputs": [],
   "source": [
    "# CTH's code\n",
    "# 第三次drop 将一些没有很少量缺失的值删除，得到一个干净的数据集\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7gkEN6pAvft"
   },
   "outputs": [],
   "source": [
    "# CTH's code\n",
    "#查看第三次drop后NA的情况\n",
    "i = 0\n",
    "j = 10\n",
    "for k in range(8):\n",
    "    df.select([count(when(isnull(c), c)).alias(c) for c in df.columns[i:j]]).show()\n",
    "    i +=10\n",
    "    j +=10\n",
    "print(df.count(),len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLlw75kbC5xM"
   },
   "outputs": [],
   "source": [
    "# CTH's code  将清理后的输入以csv格式存储在指定的地址中\n",
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbOJPr3OMmMS"
   },
   "outputs": [],
   "source": [
    "#将清洗后的数据保存到指定路径中\n",
    "df_pd.to_csv(\"/content/drive/Shared drives/718_project/dataset/listing_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5o6pzyBI907"
   },
   "source": [
    "我觉得dummy variable 的建立用010101创建很多列效率不如StringIndex高\n",
    "建议为有需要用到的categorical cols 创建 String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "tEbmronaIyW_",
    "outputId": "fc5c89a3-c8c4-461e-9330-11b3508ea7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+---------+-------------------+--------------------+------------------+------------------+-----------------+--------------------+--------------------+-------------------+-------------------------+--------------------+----------------------+--------------------+-------------+----------------------+----------------------------+-------+-----+-------+--------------+------------+-------------+--------+----------+-----------------+-------------+---------------+------------+---------+--------+----+--------+--------------------+------+---------------+------------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+--------------------+----------------+--------------------+-----------------------------+--------------------------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------------------+-----------------+-------------+------------+-----------------------+--------------------+\n",
      "|_c0|  id|         listing_url|                name|         description|experiences_offered|         picture_url|host_id|            host_url|host_name|         host_since|       host_location|host_response_time|host_response_rate|host_is_superhost|  host_thumbnail_url|    host_picture_url|host_listings_count|host_total_listings_count|  host_verifications|host_identity_verified|              street|neighbourhood|neighbourhood_cleansed|neighbourhood_group_cleansed|   city|state| market|smart_location|country_code|      country|latitude| longitude|is_location_exact|property_type|      room_type|accommodates|bathrooms|bedrooms|beds|bed_type|           amenities| price|guests_included|extra_people|minimum_nights|maximum_nights|minimum_minimum_nights|maximum_minimum_nights|minimum_maximum_nights|maximum_maximum_nights|minimum_nights_avg_ntm|maximum_nights_avg_ntm|calendar_updated|availability_30|availability_60|availability_90|availability_365|calendar_last_scraped|number_of_reviews|number_of_reviews_ltm|       first_review|        last_review|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|  jurisdiction_names|instant_bookable| cancellation_policy|require_guest_profile_picture|require_guest_phone_verification|calculated_host_listings_count|calculated_host_listings_count_entire_homes|calculated_host_listings_count_private_rooms|calculated_host_listings_count_shared_rooms|reviews_per_month|neighbourhood_group_cleansed_IDX|property_type_IDX|room_type_IDX|bed_type_IDX|cancellation_policy_IDX|calendar_updated_IDX|\n",
      "+---+----+--------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+---------+-------------------+--------------------+------------------+------------------+-----------------+--------------------+--------------------+-------------------+-------------------------+--------------------+----------------------+--------------------+-------------+----------------------+----------------------------+-------+-----+-------+--------------+------------+-------------+--------+----------+-----------------+-------------+---------------+------------+---------+--------+----+--------+--------------------+------+---------------+------------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+--------------------+----------------+--------------------+-----------------------------+--------------------------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------------------+-----------------+-------------+------------+-----------------------+--------------------+\n",
      "|  0|2318|https://www.airbn...|Casa Madrona - Ur...|Gorgeous, archite...|               none|https://a0.muscac...|   2536|https://www.airbn...|    Megan|2008-08-26 00:00:00|Seattle, Washingt...|    within an hour|             100.0|                1|https://a0.muscac...|https://a0.muscac...|                  2|                        2|['email', 'phone'...|                     0|Seattle, WA, Unit...|      Madrona|               Madrona|                Central Area|Seattle|   WA|Seattle|   Seattle, WA|          US|United States|47.61082|-122.29082|                1|        House|Entire home/apt|           9|      2.5|       4|   4|Real Bed|{Internet,Wifi,Ki...|296.00|              8|       25.00|             7|          1000|                     7|                     7|                  1000|                  1000|                   7.0|                1000.0|      5 days ago|              6|             29|             59|              59|  2019-11-21 00:00:00|               29|                    9|2008-09-15 00:00:00|2019-10-31 00:00:00|                 100|                    10|                       10|                   10|                         10|                    10|                 10|{WASHINGTON,\" Sea...|               1|strict_14_with_gr...|                            0|                               0|                             2|                                          2|                                           0|                                          0|             0.21|                             3.0|              1.0|          0.0|         0.0|                    1.0|                11.0|\n",
      "+---+----+--------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+---------+-------------------+--------------------+------------------+------------------+-----------------+--------------------+--------------------+-------------------+-------------------------+--------------------+----------------------+--------------------+-------------+----------------------+----------------------------+-------+-----+-------+--------------+------------+-------------+--------+----------+-----------------+-------------+---------------+------------+---------+--------+----+--------+--------------------+------+---------------+------------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+--------------------+----------------+--------------------+-----------------------------+--------------------------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------------------+-----------------+-------------+------------+-----------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CTH's code\n",
    "categorical_cols = ['neighbourhood_group_cleansed',\n",
    "       'property_type', 'room_type', 'bed_type','cancellation_policy','calendar_updated']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol = col + \"_IDX\")\\\n",
    "            .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "#df_categorical = df.select(categorical_cols)\n",
    "df_c_transformed = indexer_pipeline.fit(df).transform(df)\n",
    "df_c_transformed.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "idLQycfcMH9A"
   },
   "source": [
    "在通过ont hot encoding 将这个categorical variables 整合成一个vector 作为一个feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8K9-yRwXMZQr"
   },
   "outputs": [],
   "source": [
    "##CTH's code\n",
    "encoded = [OneHotEncoder(inputCol = col + \"_IDX\", outputCol = col + 'Vec') for col in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "-q86WWQoPQQV",
    "outputId": "06f54d19-8d40-4cb0-a194-3adbf2ebc97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+---------+-------------------+--------------------+------------------+------------------+-----------------+--------------------+--------------------+-------------------+-------------------------+--------------------+----------------------+--------------------+-------------+----------------------+----------------------------+-------+-----+-------+--------------+------------+-------------+--------+----------+-----------------+-------------+---------------+------------+---------+--------+----+--------+--------------------+------+---------------+------------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+--------------------+----------------+--------------------+-----------------------------+--------------------------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------------------+-----------------+-------------+------------+-----------------------+--------------------+-------------------------------+----------------+-------------+-------------+----------------------+-------------------+\n",
      "|_c0|  id|         listing_url|                name|         description|experiences_offered|         picture_url|host_id|            host_url|host_name|         host_since|       host_location|host_response_time|host_response_rate|host_is_superhost|  host_thumbnail_url|    host_picture_url|host_listings_count|host_total_listings_count|  host_verifications|host_identity_verified|              street|neighbourhood|neighbourhood_cleansed|neighbourhood_group_cleansed|   city|state| market|smart_location|country_code|      country|latitude| longitude|is_location_exact|property_type|      room_type|accommodates|bathrooms|bedrooms|beds|bed_type|           amenities| price|guests_included|extra_people|minimum_nights|maximum_nights|minimum_minimum_nights|maximum_minimum_nights|minimum_maximum_nights|maximum_maximum_nights|minimum_nights_avg_ntm|maximum_nights_avg_ntm|calendar_updated|availability_30|availability_60|availability_90|availability_365|calendar_last_scraped|number_of_reviews|number_of_reviews_ltm|       first_review|        last_review|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|  jurisdiction_names|instant_bookable| cancellation_policy|require_guest_profile_picture|require_guest_phone_verification|calculated_host_listings_count|calculated_host_listings_count_entire_homes|calculated_host_listings_count_private_rooms|calculated_host_listings_count_shared_rooms|reviews_per_month|neighbourhood_group_cleansed_IDX|property_type_IDX|room_type_IDX|bed_type_IDX|cancellation_policy_IDX|calendar_updated_IDX|neighbourhood_group_cleansedVec|property_typeVec| room_typeVec|  bed_typeVec|cancellation_policyVec|calendar_updatedVec|\n",
      "+---+----+--------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+---------+-------------------+--------------------+------------------+------------------+-----------------+--------------------+--------------------+-------------------+-------------------------+--------------------+----------------------+--------------------+-------------+----------------------+----------------------------+-------+-----+-------+--------------+------------+-------------+--------+----------+-----------------+-------------+---------------+------------+---------+--------+----+--------+--------------------+------+---------------+------------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+--------------------+----------------+--------------------+-----------------------------+--------------------------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------------------+-----------------+-------------+------------+-----------------------+--------------------+-------------------------------+----------------+-------------+-------------+----------------------+-------------------+\n",
      "|  0|2318|https://www.airbn...|Casa Madrona - Ur...|Gorgeous, archite...|               none|https://a0.muscac...|   2536|https://www.airbn...|    Megan|2008-08-26 00:00:00|Seattle, Washingt...|    within an hour|             100.0|                1|https://a0.muscac...|https://a0.muscac...|                  2|                        2|['email', 'phone'...|                     0|Seattle, WA, Unit...|      Madrona|               Madrona|                Central Area|Seattle|   WA|Seattle|   Seattle, WA|          US|United States|47.61082|-122.29082|                1|        House|Entire home/apt|           9|      2.5|       4|   4|Real Bed|{Internet,Wifi,Ki...|296.00|              8|       25.00|             7|          1000|                     7|                     7|                  1000|                  1000|                   7.0|                1000.0|      5 days ago|              6|             29|             59|              59|  2019-11-21 00:00:00|               29|                    9|2008-09-15 00:00:00|2019-10-31 00:00:00|                 100|                    10|                       10|                   10|                         10|                    10|                 10|{WASHINGTON,\" Sea...|               1|strict_14_with_gr...|                            0|                               0|                             2|                                          2|                                           0|                                          0|             0.21|                             3.0|              1.0|          0.0|         0.0|                    1.0|                11.0|                 (17,[3],[1.0])|  (26,[1],[1.0])|(4,[0],[1.0])|(5,[0],[1.0])|         (6,[1],[1.0])|    (69,[11],[1.0])|\n",
      "+---+----+--------------------+--------------------+--------------------+-------------------+--------------------+-------+--------------------+---------+-------------------+--------------------+------------------+------------------+-----------------+--------------------+--------------------+-------------------+-------------------------+--------------------+----------------------+--------------------+-------------+----------------------+----------------------------+-------+-----+-------+--------------+------------+-------------+--------+----------+-----------------+-------------+---------------+------------+---------+--------+----+--------+--------------------+------+---------------+------------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+--------------------+----------------+--------------------+-----------------------------+--------------------------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------------------+-----------------+-------------+------------+-----------------------+--------------------+-------------------------------+----------------+-------------+-------------+----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CTH's code\n",
    "encoded_pipeline = Pipeline(stages = encoded)\n",
    "df_encoded = encoded_pipeline.fit(df_c_transformed).transform(df_c_transformed)\n",
    "df_encoded.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRHb3DwYhCCI"
   },
   "outputs": [],
   "source": [
    "#文字性列的处理 CTH's code\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "df =df.withColumn('host_verifications', array(df['host_verifications']))\n",
    "df =df.withColumn('amenities', array(df['amenities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiF8gDJwyBOo"
   },
   "outputs": [],
   "source": [
    "#文字性列的处理 CTH's code\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "cv1 = CountVectorizer()\\\n",
    "    .setInputCol('host_verifications')\\\n",
    "    .setOutputCol('tf1')\n",
    "\n",
    "cv2 = CountVectorizer()\\\n",
    "    .setInputCol('amenities')\\\n",
    "    .setOutputCol('tf2')\n",
    "\n",
    "idf1 = IDF().\\\n",
    "    setInputCol(\"tf1\").\\\n",
    "    setOutputCol(\"tfidf1\")\n",
    "\n",
    "idf2 = IDF().\\\n",
    "    setInputCol(\"tf2\").\\\n",
    "    setOutputCol(\"tfidf2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJfx2QZfc3cc"
   },
   "outputs": [],
   "source": [
    "#CTH'code\n",
    "#创建随机森林回国模型\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "regression = RandomForestRegressor(labelCol='price')\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1-d1Hn-NfJi"
   },
   "outputs": [],
   "source": [
    "#cth's code\n",
    "#feature的集合\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "list_numeric = ['host_is_superhost','host_listings_count','tfidf1','tfidf2'\n",
    "                ,'host_identity_verified','accommodates','bathrooms','bedrooms'\n",
    "                ,'beds','guests_included','number_of_reviews','number_of_reviews_ltm'\n",
    "                ,'review_scores_rating','review_scores_accuracy','review_scores_cleanliness'\n",
    "                ,'review_scores_checkin','review_scores_communication','review_scores_location'\n",
    "                ,'review_scores_value','calculated_host_listings_count','calculated_host_listings_count_entire_homes'\n",
    "                ,'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms','reviews_per_month'\n",
    "                ,'neighbourhood_group_cleansedVec','property_typeVec', 'room_typeVec', 'bed_typeVec'\n",
    "                ,'cancellation_policyVec','calendar_updatedVec']\n",
    "assemble = VectorAssembler(inputCols=list_numeric,outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSBmYufidnfV"
   },
   "outputs": [],
   "source": [
    "transfomer1 = Pipeline(stages=indexers + encoded + [cv1,idf1,cv2,idf2,assemble,regression])\n",
    "transfomer2=  Pipeline(stages=indexers + encoded + [cv1,idf1,cv2,idf2,assemble,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTMgbeHfgWle"
   },
   "outputs": [],
   "source": [
    "#Cth's code\n",
    "#训练数据集，并预测价格\n",
    "df_train,df_test = df.randomSplit([0.8,0.2],seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWYgqJBrlec1"
   },
   "outputs": [],
   "source": [
    "model_forest = transfomer1.fit(df_train).transform(df_test)\n",
    "model_gbt = transfomer2.fit(df_train).transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "O4w_Cf5fqXOr",
    "outputId": "ddb9610f-34ed-4fea-c7ca-4c5fefed0048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees RMSE: 78.41470390500861\n",
      "Gradient Boosted Trees R^2: 0.7633095588808593\n",
      "Random Forest Regression RMSE: 86.41012604363834\n",
      "Random Forest Regression R^2: 0.7125813252926572\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select columns to compute test error\n",
    "evaluator = RegressionEvaluator(labelCol='price')\n",
    "# Dictionary of model predictions to loop over\n",
    "models = {'Gradient Boosted Trees': model_gbt, 'Random Forest Regression': model_forest}\n",
    "for key, preds in models.items():\n",
    "  # Create evaluation metrics\n",
    "  rmse = evaluator.evaluate(preds, {evaluator.metricName: 'rmse'})\n",
    "  r2 = evaluator.evaluate(preds, {evaluator.metricName: 'r2'})\n",
    "  \n",
    "  # Print Model Metrics\n",
    "  print(key + ' RMSE: ' + str(rmse))\n",
    "  print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CerP2MTvAa5q"
   },
   "source": [
    "## Statistic description\n",
    "## According to the host_response_rate and host_response_time columns, They have 1906 NA, we need to delete them, and draw a histgram and barplot to see the distribution of rate, we find most of hosts reply after receive messages and the time of reply is less than one hour.The mean_response_rate is 98.09%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohF0U9faAGLd"
   },
   "source": [
    "## Outlier Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPsvHuJ6pCQD"
   },
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xQnvZXYpGkl"
   },
   "outputs": [],
   "source": [
    "# import folium\n",
    "# from folium import plugins\n",
    "\n",
    "# data = Data().X.toPandas()\n",
    "\n",
    "# incidents = folium.map.FeatureGroup()\n",
    "# for lat, lng in zip(data.latitude, data.longitude):\n",
    "#     incidents.add_child(\n",
    "#       folium.CircleMarker(\n",
    "#       [lat,lng],\n",
    "#       radius=3,\n",
    "#       color='yellow',\n",
    "#       fill= True,\n",
    "#       fill_color='red',\n",
    "#       fill_opacity=0.4\n",
    "#       )\n",
    "# )\n",
    "\n",
    "seattle_map = folium.Map(location=[data.latitude.mean(), data.longitude.mean()], zoom_start=12)\n",
    "# incidents = plugins.MarkerCluster().add_to(seattle_map)\n",
    "# for lat, lng, label in zip(data.latitude, data.longitude, data.price):\n",
    "#     folium.Marker(\n",
    "#         location=[lat, lng],\n",
    "#         icon=None,\n",
    "#         popup=label).add_to(incidents)\n",
    "\n",
    "# seattle_map.add_child(incidents)\n",
    "# seattle_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v03iuh-9m1Qw"
   },
   "source": [
    "# BarPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkKtld65m_f1"
   },
   "outputs": [],
   "source": [
    "#CTH's code\n",
    "host_response_time = df_pd['host_response_time']\n",
    "print(host_response_time.value_counts())\n",
    "col = ['#FF8247','#CD919E','#7EC0EE','#8B0000','#EEEE00',]\n",
    "x = ['within an hour', 'within a few hours','within a day', 'a few days or more','N/A']\n",
    "y = [5283,630,237,34,1444]  \n",
    "plt.figure(figsize=(10,8),dpi = 80)\n",
    "plt.xlabel('Time',fontsize = 18)\n",
    "plt.ylabel('Frequency',fontsize = 18)\n",
    "plt.title('Time of Response',fontsize = 20)\n",
    "plt.bar(range(len(x)),y,width=0.5,color = col)\n",
    "plt.xticks(range(len(x)),x)\n",
    "plt.tick_params(labelsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4xzSUn_jqUm"
   },
   "outputs": [],
   "source": [
    "#CTH's code\n",
    "host_is_superhost = df_pd['host_is_superhost']\n",
    "print(host_is_superhost.value_counts())\n",
    "x = ['Superhost','Not Superhost']\n",
    "y = [3745,5274]\n",
    "plt.figure(figsize=(10,8),dpi = 80)\n",
    "plt.ylabel('Frequency',fontsize = 18)\n",
    "plt.title('Whether the host is superhost',fontsize = 20)\n",
    "plt.bar(range(len(x)),y,width=0.2,color = 'orange')\n",
    "plt.xticks(range(len(x)),x)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3osJQ7enWOi"
   },
   "outputs": [],
   "source": [
    "#CTH's code\n",
    "property_type = df_pd['property_type'].dropna(axis=0,how='any')\n",
    "property_type = property_type.value_counts()\n",
    "property_type.plot(kind='bar',figsize=(10,8), color='orange',width = 0.8)\n",
    "plt.xlabel('Property Type',fontsize = 18)\n",
    "plt.ylabel('Frequency',fontsize = 18)\n",
    "plt.title('Distribution of Property Type', fontsize = 20)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqLoKVahZkPD"
   },
   "outputs": [],
   "source": [
    "#CTH's code\n",
    "room_type = df_pd['room_type'].dropna(axis=0,how='any')\n",
    "room_type = room_type.value_counts()\n",
    "room_type.plot(kind='bar',figsize=(10,8), color='orange', width = 0.8)\n",
    "plt.xlabel('Room Type',fontsize = 18)\n",
    "plt.ylabel('Frequency',fontsize = 18)\n",
    "plt.title('Distribution of Room Type', fontsize = 20)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFIwXJz4AKF1"
   },
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-ri5RGBIuHm"
   },
   "outputs": [],
   "source": [
    "#CTH's code\n",
    "host_response_rate = df_pd['host_response_rate']\n",
    "plt.figure(figsize=(10,8),dpi = 80)\n",
    "plt.hist(host_response_rate,10,color='orange')\n",
    "plt.xlabel('host_response_rate(%)',fontsize = 18)\n",
    "plt.ylabel('frequency',fontsize = 18)\n",
    "_xtick_labels = [i for i in range(0,101)]\n",
    "plt.xticks(_xtick_labels[::10])\n",
    "plt.title('Response rate of host',fontsize = 20)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEUVITwMcpEm"
   },
   "outputs": [],
   "source": [
    "#get list of neighbourhoods\n",
    "neighbourhoods = df['neighbourhood_group_cleansed'].unique()\n",
    "\n",
    "#get prices by month and neighbourhood\n",
    "price_by_month_neighbourhood = df.groupby(['month','neighbourhood_group_cleansed']).mean().reset_index()\n",
    "\n",
    "#plot prices for each neighbourhood\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for neighbourhood in neighbourhoods:\n",
    "    ax.plot(price_by_month_neighbourhood[price_by_month_neighbourhood['neighbourhood_group_cleansed'] == neighbourhood]['month'],\n",
    "             price_by_month_neighbourhood[price_by_month_neighbourhood['neighbourhood_group_cleansed'] == neighbourhood]['price'],\n",
    "             label = neighbourhood)\n",
    "    \n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.ylabel('Average price, $')\n",
    "plt.xlabel('Month')\n",
    "plt.title('Average price for neighbourhood, $')\n",
    "\n",
    "plt.savefig('average price for neighbourhood')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcQ_Qvv_APPj"
   },
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okJwszvk7e6M"
   },
   "outputs": [],
   "source": [
    "# Mark\n",
    "data = Data().raw_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWJAJMs7Dlpv"
   },
   "outputs": [],
   "source": [
    "data.price  = data.price.dropna(axis=0,how='any')\n",
    "data.price = data.price.str.replace(',','')\n",
    "data.price = data.price.str.replace('$','')\n",
    "data.price = pd.to_numeric(data.price)\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(x=data.price )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtP0YY5NtGKy"
   },
   "outputs": [],
   "source": [
    "data.security_deposit  = data.security_deposit.dropna(axis=0,how='any')\n",
    "data.security_deposit = data.security_deposit.str.replace(',','')\n",
    "data.security_deposit = data.security_deposit.str.replace('$','')\n",
    "data.security_deposit = pd.to_numeric(data.security_deposit)\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(x=data.security_deposit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LrvgpvAtKvn"
   },
   "outputs": [],
   "source": [
    "data.cleaning_fee  = data.cleaning_fee.dropna(axis=0,how='any')\n",
    "data.cleaning_fee = data.cleaning_fee.str.replace(',','')\n",
    "data.cleaning_fee = data.cleaning_fee.str.replace('$','')\n",
    "data.cleaning_fee = pd.to_numeric(data.cleaning_fee)\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(x=data.cleaning_fee )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CELKt55iG6xM"
   },
   "outputs": [],
   "source": [
    "data = Data().raw_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUYzPjILE_8K"
   },
   "outputs": [],
   "source": [
    "price_list = ['price', 'security_deposit', 'cleaning_fee']\n",
    "for i in price_list:\n",
    "  i  = data[i].dropna(axis=0,how='any')\n",
    "  i = i.str.replace(',','')\n",
    "  i = i.str.replace('$','')\n",
    "  i = pd.to_numeric(i)\n",
    "  sns.set(style=\"whitegrid\")\n",
    "  sns.boxplot(x=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwH9VVsuCr74"
   },
   "outputs": [],
   "source": [
    "list = ['host_listings_count', \n",
    " 'host_total_listings_count', \n",
    " 'accommodates',\n",
    " 'bathrooms',\n",
    " 'bedrooms',\n",
    " 'beds',\n",
    " 'guests_included',\n",
    " 'minimum_nights',\n",
    " 'maximum_nights',\n",
    " 'minimum_minimum_nights',\n",
    " 'maximum_minimum_nights',\n",
    " 'minimum_maximum_nights',\n",
    " 'maximum_maximum_nights',\n",
    " 'minimum_nights_avg_ntm',\n",
    " 'maximum_nights_avg_ntm',\n",
    " 'number_of_reviews',\n",
    " 'number_of_reviews_ltm',\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdiW7s64KHn0"
   },
   "outputs": [],
   "source": [
    "for i in list:\n",
    "  title = i\n",
    "  i = data[i].dropna(axis=0,how='any')\n",
    "  sns.set(style=\"whitegrid\")\n",
    "  sns.boxplot(x=i)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lh2jSQWyARMX"
   },
   "source": [
    "### PairPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GryukAScATeQ"
   },
   "source": [
    "## Correlation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGwLzUTVXDe5"
   },
   "outputs": [],
   "source": [
    "data = Data().raw_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6PxTmYwWVJo"
   },
   "outputs": [],
   "source": [
    "#col_list = []\n",
    "sns.heatmap()\n",
    "cols = ['accommodates','bathrooms','bedrooms','beds','host_since_year',\n",
    "        'host_listings_count', 'extra_people_fee',\n",
    "        'review_scores_rating', 'price']\n",
    "\n",
    "#Find out correlation between columns and plot\n",
    "corrs = np.corrcoef(df[cols].values.T)\n",
    "sns.set(font_scale=1)\n",
    "sns.set(rc={'figure.figsize':(7,7)})\n",
    "hm=sns.heatmap(corrs, cbar = True, annot=True, square = True, fmt = '.2f',\n",
    "              yticklabels = cols, xticklabels = cols).set_title('Correlations heatmap')\n",
    "\n",
    "fig = hm.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxRCVKlfAaDj"
   },
   "source": [
    "## Domain knowledge, knowledge research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptn_n9fR__13"
   },
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WX2u5cCUsd1K"
   },
   "source": [
    "Constructing analyzing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smBu9IdIsW-Y"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "data = Data()\n",
    "train_X = data.train_X\n",
    "train_Y = data.test_Y\n",
    "print(train_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4tQuI0v9ngU"
   },
   "source": [
    "自定义管道，可用作验证时管道的复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpA3ZTb-uApe"
   },
   "outputs": [],
   "source": [
    "# 这是个自定义管道的例子\n",
    "class CustomTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhIRCVAs93JM"
   },
   "source": [
    "总体分析步骤，你们挑感兴趣的步骤写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On-c8ZvipC4E"
   },
   "outputs": [],
   "source": [
    "class LowVarianceFilter(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jm1otRkJ-K-8"
   },
   "source": [
    "空值分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsf0dUJj-Ihb"
   },
   "outputs": [],
   "source": [
    "class MissingValueTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ek8TH_b-Xjg"
   },
   "source": [
    "字段筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_Kj6I_c-Jnr"
   },
   "outputs": [],
   "source": [
    "class DropColumnTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bh1uuMqF-jh9"
   },
   "source": [
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zq3fjr-K-mxw"
   },
   "outputs": [],
   "source": [
    "# 这个其实官方库已经有了，只需要找到网上的例子搬过来\n",
    "class OneHotTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmgzTn_q-vlk"
   },
   "source": [
    "Outlier Handling\n",
    "1. Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrlTmqP6_UvG"
   },
   "outputs": [],
   "source": [
    "class OutlierDropTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p857XERv-3tn"
   },
   "source": [
    "2. Replace with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RP2L76BV_XZ7"
   },
   "outputs": [],
   "source": [
    "class OutlierReplaceMeanTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzr7oRwJ-8SN"
   },
   "source": [
    "3. Replace with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDnN8YB9_f6Z"
   },
   "outputs": [],
   "source": [
    "class OutlierReplaceMedianTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGJR24zl_ATM"
   },
   "source": [
    "4. Randomly replae with new value(using normal distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-FEwTf6A1y0"
   },
   "source": [
    "5. Logarithm encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DF6Gj7p0_iJw"
   },
   "outputs": [],
   "source": [
    "class OutlierReplaceRandomNormalDistTransformer(Transformer):\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db1NIP60_o9D"
   },
   "source": [
    "Scaling\n",
    "1. Min-max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CHSiKp6_swx"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "#scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "#scalerModel = scaler.fit(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xWuOpQ0_u4H"
   },
   "source": [
    "2. Standardize Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmJr8q8M_xjV"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    " #                       withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMil45m__THg"
   },
   "source": [
    "Build actual pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-kVM38799aL"
   },
   "outputs": [],
   "source": [
    "trans_pipe = Pipeline(stages=[\n",
    "    LowVarianceFilter(),\n",
    "    # MissingValueTransformer(),\n",
    "    # feature.VectorAssembler(inputCols=['col1', 'col2', 'col3', 'col4'], outputCol='features'),\n",
    "    # regression.LinearRegression(featuresCol='features', labelCol='abc')\n",
    "])\n",
    "\n",
    "display(trans_pipe.fit(train_X).transform(train_X).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8maFNWTuMPbQ"
   },
   "outputs": [],
   "source": [
    "train_X.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Wz95ql3K23d"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJSDpPiiBNH-"
   },
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mOjjOc2yrDo"
   },
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udPJP-gzyqJX"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtO_1ejFyqCa"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeH6aPY8BT4K"
   },
   "source": [
    "# Concolusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsR_UvWfMKDy"
   },
   "source": [
    "\\begin{equation} L\\theta^{\\lambda}(p(X),Y) = -\\left( \\sum_i Y_i \\log p\\theta(Xi) + (1-Y_i)\\log(1-p\\theta(Xi)) \\right) + \\lambda \\sum{j>0} \\left| \\theta_j \\right| \\end{equation}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OeH6aPY8BT4K"
   ],
   "name": "718_project",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
