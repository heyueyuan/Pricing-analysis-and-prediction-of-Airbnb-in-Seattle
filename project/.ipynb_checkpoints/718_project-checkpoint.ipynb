{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb price analysis and prediction in Seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 988kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/mark/anaconda3/lib/python3.7/site-packages (from langdetect) (1.12.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.8-cp37-none-any.whl size=993193 sha256=e79067869b87fd3ed733890e34d8e7f4efa45245c71d9b104d461756262c7f9d\n",
      "  Stored in directory: /Users/mark/Library/Caches/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.8\n",
      "Requirement already satisfied: nltk in /Users/mark/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/mark/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Collecting wordcloud\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/67/444d4ccf5cc5058e7d3137ba5a491c06a8430c50ab64dbb761bd532f04a8/wordcloud-1.6.0-cp37-cp37m-macosx_10_6_x86_64.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 987kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /Users/mark/anaconda3/lib/python3.7/site-packages (from wordcloud) (1.18.2)\n",
      "Requirement already satisfied: pillow in /Users/mark/anaconda3/lib/python3.7/site-packages (from wordcloud) (7.0.0)\n",
      "Requirement already satisfied: matplotlib in /Users/mark/anaconda3/lib/python3.7/site-packages (from wordcloud) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mark/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mark/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/mark/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/mark/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: six in /Users/mark/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/mark/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.6.0\n",
      "Collecting singleton_decorator\n",
      "  Downloading https://files.pythonhosted.org/packages/33/98/a8b5c919bee1152a9a1afd82014431f8db5882699754de50d1b3aba4d136/singleton-decorator-1.0.0.tar.gz\n",
      "Building wheels for collected packages: singleton-decorator\n",
      "  Building wheel for singleton-decorator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for singleton-decorator: filename=singleton_decorator-1.0.0-cp37-none-any.whl size=3124 sha256=827112637322cd411dd034d925d561b5baeb0aa96c6cfcb24a75ce048cfd587b\n",
      "  Stored in directory: /Users/mark/Library/Caches/pip/wheels/ce/43/87/9c4d65e727c32931aca54674ccf4f204e664306fbb507bcbd2\n",
      "Successfully built singleton-decorator\n",
      "Installing collected packages: singleton-decorator\n",
      "Successfully installed singleton-decorator-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install singleton_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "# grading import statements\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.crossJoin.enabled\",\"true\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "# Correct Usage Example (pass ONLY the full file name):\n",
    "#   file_name_to_load = get_training_filename(\"sms_spam.csv\") # correct - pass ONLY the full file name  \n",
    "#   \n",
    "# Incorrect Usage Example\n",
    "#   file_name_to_load = get_training_filename(\"/sms_spam.csv\") # incorrect - pass ONLY the full file name\n",
    "#   file_name_to_load = get_training_filename(\"sms_spam.csv/\") # incorrect - pass ONLY the full file name\n",
    "#   file_name_to_load = get_training_filename(\"c:/users/will/data/sms_spam.csv\") incorrect -pass ONLY the full file name\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcZmk0xfxLah"
   },
   "source": [
    "Load Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "RaY1mvHzxCT2",
    "outputId": "083e680e-d46b-4cda-f52e-3591a74ee0cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPF7YNEcndbX"
   },
   "source": [
    "Create connect to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5_XK8zw0zem"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from collections.abc import Iterable\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "seed = 77\n",
    "\n",
    "def load_data(file_name, sampling=False):\n",
    "  df = spark.read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"multiLine\", \"true\")\\\n",
    "        .option('inferSchema', 'true')\\\n",
    "        .option('escape', '\"')\\\n",
    "        .csv(get_training_filename(file_name))\n",
    "  if sampling:\n",
    "    df = df.sample(True, .2, seed=seed)\n",
    "  return df\n",
    "        \n",
    "\n",
    "def split_data(raw_data, training_rate=0.7):\n",
    "  train_data, test_data = raw_data.randomSplit([training_rate, 1-training_rate])\n",
    "  return (train_data, test_data)\n",
    "\n",
    "def shape(df):\n",
    "  return (df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2ApdKLXnoed"
   },
   "source": [
    "Create data object, fill the raw/train/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkTfWj7V_5rQ"
   },
   "source": [
    "# EDA and Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6zcGsLil9My"
   },
   "source": [
    "We will explore the whole dataset during the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icMTaZsEl9y1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "df = load_data('listings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FP8OVmOhyrs9"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBC3NWkGy7SD"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/mark/OneDrive - Syracuse University/Courses-SYR/IST-718/project/reviews.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o63.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/mark/OneDrive - Syracuse University/Courses-SYR/IST-718/project/reviews.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:617)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b61c7a92ffec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reviews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-0c61e9e8ff32>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_name, sampling)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inferSchema'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'escape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_training_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/mark/OneDrive - Syracuse University/Courses-SYR/IST-718/project/reviews.csv;'"
     ]
    }
   ],
   "source": [
    "df_reviews = load_data('reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtWqVHe0zKO8"
   },
   "source": [
    "### Who likes Airbnb most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "gqZkDY5Gzjai",
    "outputId": "505f750b-0a7f-4f77-b4e5-ce9074a16041"
   },
   "outputs": [],
   "source": [
    "most_person = df_reviews.groupBy('reviewer_name').count().sort(\"count\", ascending=False)\n",
    "most_person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "wDIzQHma6NbS",
    "outputId": "4e672270-6bcf-4bf8-ec5a-56df16b7d4dc"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3), dpi=100)\n",
    "sns.barplot(\"reviewer_name\",\"count\",palette=\"RdBu_r\",data=most_person.toPandas().head(20))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94DVmDYfsmxA"
   },
   "source": [
    "### Which house is the most popular？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "b9iQO00Pzj8i",
    "outputId": "7ff3724f-d8ac-4409-fb03-1eaefe2bf41e"
   },
   "outputs": [],
   "source": [
    "most_living = df_reviews.groupBy('listing_id').count().sort(\"count\", ascending=False)\n",
    "most_living.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "rWx1tVB46tj3",
    "outputId": "f385f1b4-91d9-4619-f38b-3fc7a429d423"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'most_living' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0b0803960c31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmost_living_df\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmost_living\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"listing_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"husl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmost_living_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'most_living' is not defined"
     ]
    }
   ],
   "source": [
    "most_living_df  = most_living.toPandas()\n",
    "fig = plt.figure(figsize=(12, 3), dpi=100)\n",
    "sns.barplot(\"listing_id\",\"count\",palette=\"husl\",data=most_living_df.head(20),order=None)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0egzZ2N3dYR"
   },
   "source": [
    "### Whose house is the most popular？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y13HMn7W6axo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = df.select(['id','listing_url','host_name','neighbourhood_group_cleansed'])\n",
    "df2 = df2.withColumn('listing_id', fn.regexp_replace(fn.col('listing_url'), \"https://www.airbnb.com/rooms/\" , '' ).cast('int'))\n",
    "most_living = most_living.join(df2,\"listing_id\", \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "kUVqw3p0ulhg",
    "outputId": "a9bef9a6-6063-4229-d5fb-aa937bf0f81d"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6), dpi=100)\n",
    "sns.barplot(y = \"host_name\",x = \"count\",palette=\"husl\",data=most_living.toPandas().head(20))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-j8Abrc3kXd"
   },
   "source": [
    "### Where is the most popular house？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "gqLpsl6a1_Tv",
    "outputId": "d132c66b-a162-4e9d-ae4c-4fb02b3123c8"
   },
   "outputs": [],
   "source": [
    "location_groupby = most_living.groupby('neighbourhood_group_cleansed').count()\n",
    "location_groupby_sorted = location_groupby.orderBy('count', ascending=False)\n",
    "fig = plt.figure(figsize=(12, 3), dpi=100)\n",
    "sns.barplot(x = 'neighbourhood_group_cleansed', y = 'count',data=location_groupby_sorted.toPandas(),saturation=1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hlwJ8OH4XVR"
   },
   "source": [
    "## Numerical Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2AzLRdl5lmT"
   },
   "source": [
    "### Convert values in columns from string to number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwoEvaeM5yoC"
   },
   "source": [
    "- Display the type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vAfLBuhD4e5b",
    "outputId": "80d56735-e9dc-4706-c41f-716f5423fefc"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZHhi2LhLt26A",
    "outputId": "4b8d8cd5-d342-4a64-8b35-b5a98ae66d30"
   },
   "outputs": [],
   "source": [
    "shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF-qQx9qtvq4"
   },
   "source": [
    "- It is a 9023 by 106 data dataset. The dataset are comprised of categorical(ordinal, nominal) variables, numerical(integer, float) variables, timestamp variable, and boolean variables. \n",
    "- Some of the features are loaded with incorrect format. \n",
    "- The follow up process will impute the problematic data into a purified dataset by using reg expression, winsorizing, and logarthim transformation.\n",
    "- The review comments resides in some of the variables. This analysis will foucs on quantitative reasoning. So we will disregard those features. \n",
    "- There are 106 columns in the dataset. According to the 'curse of the dimensionality', feature selection is necessary. After data cleaning, Lasso regression and Random Forest will be applied to the dataset to select the important feature as the columns of the training set. \n",
    "- Since the dataset is huge in terms of the number of feature, we will sampling a subset to explore the data space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LjlFoBBkD8P8",
    "outputId": "cd94e8ee-a12a-4d24-c518-540beb934caf"
   },
   "outputs": [],
   "source": [
    "df = load_data('listings.csv', True)\n",
    "shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hORW6ywl6LSz"
   },
   "source": [
    "- Select boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "CA6SxxOz6NbX",
    "outputId": "8179739c-f954-4b4d-dcd7-ee0284a0c166"
   },
   "outputs": [],
   "source": [
    "bool_columns = ['host_is_superhost', \n",
    "                    'host_has_profile_pic', \n",
    "                    'host_identity_verified', \n",
    "                    'is_location_exact', \n",
    "                    'has_availability', \n",
    "                    'requires_license', \n",
    "                    'instant_bookable', \n",
    "                    'is_business_travel_ready', \n",
    "                    'require_guest_profile_picture', \n",
    "                    'require_guest_phone_verification']\n",
    "df_bool = df.select(bool_columns)\n",
    "df_bool.limit(5).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlWlQqqRB09H"
   },
   "source": [
    "- Replace t to 1, and f to 0 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "zWvZm7saB5-F",
    "outputId": "346029e9-5ff2-4a7e-ac56-24222ce82d28"
   },
   "outputs": [],
   "source": [
    "from singleton_decorator import singleton\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "\n",
    "@singleton\n",
    "class BooleanConverter(Transformer):\n",
    "  def __init__(self):\n",
    "    self._bool_columns = ['host_is_superhost', \n",
    "                    'host_has_profile_pic', \n",
    "                    'host_identity_verified', \n",
    "                    'is_location_exact', \n",
    "                    'has_availability', \n",
    "                    'requires_license', \n",
    "                    'instant_bookable', \n",
    "                    'is_business_travel_ready', \n",
    "                    'require_guest_profile_picture', \n",
    "                    'require_guest_phone_verification']\n",
    "    self._bool_dict = {'t': 1, 'f': 0}\n",
    "    def bool_map(x):\n",
    "      if x in self._bool_dict.keys():\n",
    "        return self._bool_dict[x]\n",
    "      return x\n",
    "    self._bool_encode_udf = fn.udf(bool_map, t.IntegerType())\n",
    "\n",
    "  @property\n",
    "  def bool_columns(self):\n",
    "    return self._bool_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "    for col_name in self._bool_columns:\n",
    "      if col_name in result.columns:\n",
    "        result = result.withColumn(col_name, self._bool_encode_udf(fn.col(col_name)))\n",
    "    return result\n",
    "\n",
    "bool_converter = BooleanConverter()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "result.select([fn.col(col_name) for col_name in bool_converter.bool_columns]).limit(5).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNA7pgs2O_gF"
   },
   "source": [
    "### Convert values in columns from formatted string to number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUm23gfFFgDu"
   },
   "source": [
    "- Select price formatted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "FMfX7GY8Petn",
    "outputId": "6ae0c268-ba1f-4ad2-83f5-c3b45ec0ccb6"
   },
   "outputs": [],
   "source": [
    "price_columns = ['extra_people', \n",
    "                 'price']\n",
    "df_price = df.select(price_columns)\n",
    "df_price.limit(5).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfgT5TAZRkDr"
   },
   "source": [
    "- Reformat the currency formattet to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "fNEtgXuSRtT8",
    "outputId": "d0c487d2-8fc6-4482-dfaf-3e3fef28eae4"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class CurrencyConverter(Transformer):\n",
    "  def __init__(self):\n",
    "    self._price_columns = [\n",
    "                           'extra_people', \n",
    "                           'price', \n",
    "                           'weekly_price', \n",
    "                           'monthly_price', \n",
    "                           'security_deposit', \n",
    "                           'cleaning_fee', \n",
    "                           ]\n",
    "  @property\n",
    "  def price_columns(self):\n",
    "    return self._price_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "    for col_name in self._price_columns:\n",
    "      if col_name in result.columns:\n",
    "        result = result.withColumn(col_name, fn.regexp_replace(fn.col(col_name), \"\\$|,\" , '' ).cast('double'))\n",
    "    return result\n",
    "\n",
    "\n",
    "currency_converter = CurrencyConverter()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "result.select([fn.col(col_name) for col_name in currency_converter.price_columns]).limit(5).toPandas().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsSdW1slk6fc"
   },
   "source": [
    "## Revise incorrect string-type columns into numerical one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "IsG5VfvQk5xx",
    "outputId": "e5a73005-5259-4c13-9c7a-6219b0ecbc9f"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class RateConverter(Transformer):\n",
    "  def __init__(self):\n",
    "    self._rate_columns = [\n",
    "                           'host_response_rate', \n",
    "                           'host_acceptance_rate', \n",
    "                           ]\n",
    "  @property\n",
    "  def rate_columns(self):\n",
    "    return self._rate_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "    for col_name in self._rate_columns:\n",
    "      if col_name in result.columns:\n",
    "        result = result.withColumn(col_name, fn.regexp_replace(fn.col(col_name), \"\\%\" , '' ).cast('double'))\n",
    "    return result\n",
    "\n",
    "\n",
    "rate_converter = RateConverter()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "result.select([fn.col(col_name) for col_name in rate_converter.rate_columns]).limit(5).toPandas().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4OV8u-A-P7P"
   },
   "source": [
    "## Zero Variance Variables Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "2r72UtqOzyve",
    "outputId": "3307f8c0-f2bb-4a2d-9b7d-442a7840ed9e"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "@singleton\n",
    "class ZeroVarianceCleaner(Transformer):\n",
    "  '''\n",
    "    Not threadsafe\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    self._reg_exp = re.compile(\"variance\\((.*)\\)\", re.IGNORECASE)\n",
    "    self._zero_variance_columns = list()\n",
    "\n",
    "  @property\n",
    "  def zero_variance_columns(self):\n",
    "    return self._zero_variance_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "    if len(numerical_columns) > 0:\n",
    "      numerical_columns = [col_name for col_name in numerical_columns if col_name not in ['id', 'longtitude', 'latitude']]\n",
    "      numeric_variances = result.agg({col_name : 'variance' for col_name in numerical_columns})\n",
    "      numeric_variances = numeric_variances.select(*[fn.col(col).alias(self._reg_exp.search(col).group(1)) for col in numeric_variances.columns])\n",
    "      numeric_variances = numeric_variances.toPandas().T.iloc[:,0]\n",
    "      numeric_variances = numeric_variances[(numeric_variances == 0) | (numeric_variances.isna())]\n",
    "      self._zero_variance_columns = [*self._zero_variance_columns, *numeric_variances.index]\n",
    "\n",
    "    string_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.StringType)]\n",
    "    if len(string_columns) > 0:\n",
    "      string_columns_unique_count = result.agg(*(fn.countDistinct(fn.col(col_name)).cast('int').alias(col_name) for col_name in string_columns))\n",
    "      string_columns_unique_count = string_columns_unique_count.toPandas().T.iloc[:,0]\n",
    "      string_columns_unique_count = string_columns_unique_count[(string_columns_unique_count < 2) | (string_columns_unique_count.isna())]\n",
    "      self._zero_variance_columns = [*self._zero_variance_columns, *string_columns_unique_count.index]\n",
    "\n",
    "    timestamp_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.TimestampType)]\n",
    "    if len(timestamp_columns) > 0:\n",
    "      timestamp_columns_unique_count = result.agg(*(fn.countDistinct(fn.col(col_name)).cast('int').alias(col_name) for col_name in timestamp_columns))\n",
    "      timestamp_columns_unique_count = timestamp_columns_unique_count.toPandas().T.iloc[:,0]\n",
    "      timestamp_columns_unique_count = timestamp_columns_unique_count[(timestamp_columns_unique_count < 2) | (timestamp_columns_unique_count.isna())]\n",
    "      self._zero_variance_columns = [*self._zero_variance_columns, *timestamp_columns_unique_count.index]\n",
    "\n",
    "    if len(self._zero_variance_columns) > 0:\n",
    "      result = result.drop(*self._zero_variance_columns)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "zero_variance_cleaner = ZeroVarianceCleaner()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "pd.DataFrame({'Removed': [col_name not in result.columns for col_name in zero_variance_cleaner.zero_variance_columns]}, index = zero_variance_cleaner._zero_variance_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gf0IzMXrxmII"
   },
   "source": [
    "## Drop descriptive columns\n",
    "- The sentiment analysis will not be a part of linear or tree based machine learning algorithm, so just drop descriptive comment columns directly. \n",
    "- Latter these features will go in to a separate dataset to train sentiment analysis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "SuIPFENdxwK3",
    "outputId": "a2a41a80-80e6-44d4-f5f4-2be2ae2edf31"
   },
   "outputs": [],
   "source": [
    "class ColumnRemover(Transformer):\n",
    "  '''\n",
    "    Not threadsafe\n",
    "  '''\n",
    "  def __init__(self, drop_columns):\n",
    "    self._drop_columns = drop_columns\n",
    "\n",
    "  @property\n",
    "  def drop_columns(self):\n",
    "    return self._drop_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    result = result.drop(*[col_name for col_name in self._drop_columns if col_name in result.columns])\n",
    "\n",
    "    return result\n",
    "\n",
    "literature_column_remover = ColumnRemover([\n",
    "                                'name', 'summary', 'space', 'description', \n",
    "                                'neighborhood_overview', 'notes', 'transit', \n",
    "                                'access', 'interaction', 'house_rules', \n",
    "                                'host_name', 'host_about', 'jurisdiction_names'])\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "pd.DataFrame({'Removed': [col_name not in result.columns for col_name in literature_column_remover.drop_columns]}, index = literature_column_remover.drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAPZ4HOD1LmQ"
   },
   "source": [
    "## Drop URL columns\n",
    "- The url scraping analysis is out of the report scope, drop these url related column accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "UxMGIrn91Uwv",
    "outputId": "8256339d-9eb9-4781-eabc-cb7bac6ebe1b"
   },
   "outputs": [],
   "source": [
    "url_column_remover = ColumnRemover(['listing_url', 'picture_url', 'host_url', 'host_thumbnail_url', 'host_picture_url'])\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "pd.DataFrame({'Removed': [col_name not in result.columns for col_name in url_column_remover.drop_columns]}, index = url_column_remover.drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1bMFXVmmA_v"
   },
   "source": [
    "## Missing value exploratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_6dWZ3LmBR-"
   },
   "source": [
    "- Identify the number of missing value at each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "I84RLe3hmEiL",
    "outputId": "d55f3a6f-bdd0-48d7-f75e-dad5d3c3e484"
   },
   "outputs": [],
   "source": [
    "def calc_missing_ratio(data_frame: DataFrame):\n",
    "  record_cnt = data_frame.count()\n",
    "  df_columns = data_frame.columns\n",
    "  \n",
    "  df_result = data_frame.select([fn.col(c).cast(t.StringType()) for c in data_frame.columns]) \\\n",
    "    .select([fn.sum(fn.when(fn.isnull(c), 1).otherwise(0)).alias(c) for c in data_frame.columns]) \\\n",
    "    .select([(col(c)/record_cnt).alias(c) for c in data_frame.columns]) \\\n",
    "    .toPandas().T\n",
    "\n",
    "  df_result = df_result.loc[(df_result != 0).all(axis=1), :]\n",
    "  df_result.columns = ['Missing Value Ratio']\n",
    "  df_result.sort_values(by=['Missing Value Ratio'], ascending=False, inplace=True)\n",
    "\n",
    "  return df_result\n",
    "\n",
    "missing_ratio = calc_missing_ratio(result)\n",
    "\n",
    "print(missing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ge9TujRVmL9E"
   },
   "source": [
    "- Visualize the ratio of missing values using bar plot.<br>\n",
    "We will focus on the columns that has more than 2% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "8nj1AQqpmO-d",
    "outputId": "4ace0fb4-0488-4dcc-f2fc-02a16f2600ec"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "def plot_missing_freq(df, xlab, ylab, title):\n",
    "  plt.clf()\n",
    "  plt.figure(figsize=(20, 5))\n",
    "  df = pd.Series(df.iloc[:, 0].values, index=list(df.index))\n",
    "\n",
    "  ax = sns.barplot(x=df.values * 100, y=df.index, orient='h')\n",
    "  \n",
    "  ax.set_xticklabels(df.values * 100, fontsize=15)\n",
    "  ax.set_yticklabels(df.index, fontsize=12)\n",
    "  ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "  ax.set_xlabel(xlab, fontsize=15)\n",
    "  ax.set_ylabel(ylab, fontsize=35)\n",
    "\n",
    "  plt.title(title, fontsize=20)\n",
    "  # for bar in ax.patches:\n",
    "  #   bar.set_height(30)\n",
    "\n",
    "  display()\n",
    "\n",
    "missing_ratio_2 = calc_missing_ratio(result)\n",
    "missing_ratio_2 = missing_ratio_2.loc[(missing_ratio_2 > 0.02).all(axis=1), :]\n",
    "plot_missing_freq(missing_ratio_2, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rzOwPwu7ENd"
   },
   "source": [
    "## Drop columns that have over half missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "zgjMCT7O7Dte",
    "outputId": "e087f4a0-9e94-4f38-9eb9-33f0516a5082"
   },
   "outputs": [],
   "source": [
    "incomplete_column_remover = ColumnRemover(['square_feet', 'monthly_price', 'weekly_price', 'license'])\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "ColumnRemover\n",
    "pd.DataFrame({'Removed': [col_name not in result.columns for col_name in incomplete_column_remover.drop_columns]}, index = incomplete_column_remover.drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xEbjECw8s6z"
   },
   "source": [
    "## Drop redundant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpn5DhFW8iFM"
   },
   "source": [
    "There are some columns that use different values expressing the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "H2x_Mw7D88Vb",
    "outputId": "eabd950f-8348-4085-8623-2645a8eab57c"
   },
   "outputs": [],
   "source": [
    "string_columns = result.select(*[fn.col(f.name) for f in result.schema.fields if isinstance(f.dataType, t.StringType)]).columns\n",
    "categorical_cols = ['neighbourhood_group_cleansed','host_response_time',\n",
    "       'property_type', 'room_type', 'bed_type','cancellation_policy']\n",
    "tfidf_cols = ['host_verifications','amenities']\n",
    "string_columns_filtered = [c for c in string_columns if ( c not in categorical_cols + tfidf_cols )]\n",
    "result.agg(*(fn.countDistinct(fn.col(col_name)).cast('int').alias(col_name) for col_name in string_columns_filtered)).toPandas().T.iloc[:, 0].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deVulQnv-Nes"
   },
   "source": [
    "Word cloud helps to identify the diversity of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM-C4bhlG5n-"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Encoding space to underscore, filter out zipcode \n",
    "words = result.select(*string_columns)\n",
    "for col_name in string_columns:\n",
    "      words = words.withColumn(col_name, fn.regexp_replace(fn.col(col_name), ' ' , '_' )).withColumn(col_name, fn.lower(fn.col(col_name)))\n",
    "words = words.drop('zipcode')\n",
    "\n",
    "cloud_list = list()\n",
    "for col_name in words.columns:\n",
    "  cloud_list.append(words.agg(fn.concat_ws(' ', fn.collect_list(col_name))).toPandas().iloc[0,0])\n",
    "cloud_list = pd.Series(cloud_list, index=words.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3bXDt3jV-OF6",
    "outputId": "5b46f179-696a-4262-be1e-e934c434a304"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "f, ax = plt.subplots(4, 4, figsize=(120,120))\n",
    "for i in range(4):\n",
    "  for j in range(4):\n",
    "    ws = cloud_list.iloc[4 * i + j]\n",
    "    title = cloud_list.index[4 * i + j]\n",
    "    ax[i, j].imshow(WordCloud(max_font_size=240, width = 240, height = 240, background_color='white').generate(ws))\n",
    "    ax[i, j].set_title(title, fontsize=100)\n",
    "    ax[i, j].axis(\"off\") \n",
    "\n",
    "plt.show()\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "g64VnefgTtaX",
    "outputId": "8d02ccaa-c3b8-41fc-dfd4-7706a92e10bc"
   },
   "outputs": [],
   "source": [
    "sentement_analysis_column_remover = ColumnRemover(string_columns_filtered)\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "pd.DataFrame({'Removed': [col_name not in result.columns for col_name in sentement_analysis_column_remover.drop_columns]}, index = sentement_analysis_column_remover.drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwLlrKisZjdA"
   },
   "source": [
    "## Second round missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14RUHQafaiGt"
   },
   "source": [
    "### Inspect ratio of missing value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "ZeM7AAyVYmfd",
    "outputId": "57c8f3bc-c559-49af-c4af-4d008cd5cd4b"
   },
   "outputs": [],
   "source": [
    "missing_ratio_3 = calc_missing_ratio(result)\n",
    "missing_ratio_3 = missing_ratio_3.loc[(missing_ratio_3 > 0.02).all(axis=1), :]\n",
    "plot_missing_freq(missing_ratio_3, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uup5nxqCxedw"
   },
   "source": [
    "### Drop sentiment comment related columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16iQcUntdhqT"
   },
   "source": [
    "- It is possible that review related values missing are due to lacking of guest, to aviod bias, the records that has no review related data will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tpUS4F-Bd89M",
    "outputId": "75b0383d-0afa-4037-be12-3105940094f1"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class MissingReviewRecordRemover(Transformer):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    review_columns = [*[col_name for col_name in result.columns if col_name.startswith('review')], 'id']\n",
    "    drop_rate = (1 - result.select(review_columns).dropna().count() / result.count()) * 100\n",
    "    review_dropped_result = result.select(review_columns).dropna().select(fn.col('id').alias('_id'))\n",
    "    result = review_dropped_result.join(result, review_dropped_result._id == result.id).drop('_id')\n",
    "\n",
    "    return result\n",
    "\n",
    "missing_review_record_remover = MissingReviewRecordRemover()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "shape(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "colab_type": "code",
    "id": "axldrk35bYGX",
    "outputId": "17a4ff36-312b-4394-daac-1330f14a59fe"
   },
   "outputs": [],
   "source": [
    "missing_ratio_4 = calc_missing_ratio(result)\n",
    "plot_missing_freq(missing_ratio_4, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ym3LTQNHxtBk"
   },
   "source": [
    "## Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6sP0FaXkdNT"
   },
   "source": [
    "- Impute the rest missing values with mean for numerical columns and mode for boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZwuYS9DqQIcA",
    "outputId": "707d86f4-2fd2-4afb-a823-644c1fff3ef3"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class MissingValueImputer(Transformer):\n",
    "  def __init__(self):\n",
    "    self._reg_exp = re.compile(\"avg\\((.*)\\)\", re.IGNORECASE)\n",
    "    self._reg_exp_2 = re.compile(\"mode\\((.*)\\)\", re.IGNORECASE)\n",
    "    self._excuded_columns = ['id', 'host_id', 'host_is_superhost', 'longitude', 'latitude', 'is_location_exact',\n",
    "                             'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "    self._boolean_columns = ['host_is_superhost', 'is_location_exact', 'instant_bookable', \n",
    "                       'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "    numerical_columns = [col_name for col_name in numerical_columns if col_name not in self._excuded_columns]\n",
    "\n",
    "    means = result.agg({col_name: 'mean' for col_name in numerical_columns})\n",
    "    for col_name in means.columns:\n",
    "      means = means.withColumnRenamed(col_name, f'_{self._reg_exp.search(col_name).group(1)}')\n",
    "\n",
    "    dummy_link = fn.udf(lambda x:1, t.IntegerType())\n",
    "    result = result.withColumn('dummy_link', dummy_link(result.columns[0]))\n",
    "    means = means.withColumn('dummy_link', dummy_link(means.columns[0]))\n",
    "    result = result.join(means, on='dummy_link', how='inner')\n",
    "\n",
    "    for col_name in numerical_columns:\n",
    "      result = result.withColumn(col_name, fn.coalesce(fn.col(col_name), fn.col(f'_{col_name}')))\n",
    "    mode_map = dict()\n",
    "    for col_name in self._boolean_columns:\n",
    "      if col_name not in result.columns:\n",
    "        continue\n",
    "      cnts = result.groupBy(col_name).count()\n",
    "      mode = cnts.join(\n",
    "          cnts.agg(fn.max(\"count\").alias(\"max_\")), fn.col(\"count\") == fn.col(\"max_\")\n",
    "      ).limit(1).select(col_name)\n",
    "      mode = mode.first()[0]\n",
    "      mode_map[col_name]=mode\n",
    "\n",
    "    result = result.drop(*means.columns)\n",
    "    result = result.fillna(mode_map)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "missing_value_imputer = MissingValueImputer()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer])\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "print(f'Number of remaining value: {result.count() - result.dropna().count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRixyNQ6lpwM"
   },
   "source": [
    "- Remove the 4 missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGLUIWH9luKi"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class MissingValueImputer(Transformer):\n",
    "  def __init__(self):\n",
    "    self._reg_exp = re.compile(\"avg\\((.*)\\)\", re.IGNORECASE)\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "\n",
    "    means = result.agg({col_name: 'mean' for col_name in numerical_columns})\n",
    "    for col_name in means.columns:\n",
    "      means = means.withColumnRenamed(col_name, f'_{self._reg_exp.search(col_name).group(1)}')\n",
    "\n",
    "    dummy_link = fn.udf(lambda x:1, t.IntegerType())\n",
    "    result = result.withColumn('dummy_link', dummy_link(result.columns[0]))\n",
    "    means = means.withColumn('dummy_link', dummy_link(means.columns[0]))\n",
    "    result = result.join(means, on='dummy_link', how='inner')\n",
    "\n",
    "    for col_name in numerical_columns:\n",
    "      result = result.withColumn(col_name, fn.coalesce(fn.col(col_name), fn.col(f'_{col_name}')))\n",
    "    result = result.drop(*means.columns).dropna()\n",
    "\n",
    "    return result\n",
    "\n",
    "missing_value_imputer = MissingValueImputer()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer])\n",
    "result = wrangling_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "DQxkl0Lxl9Lx",
    "outputId": "82b6cc0e-ec0a-4589-b3f1-92c59c1d2b70"
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wnq52i_xIGz"
   },
   "outputs": [],
   "source": [
    "id_date = ['id','host_since','host_id','first_review','last_review','_c0']\n",
    "id_date_column_remover = ColumnRemover(id_date)\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer\n",
    "                            ,id_date_column_remover])\n",
    "result = wrangling_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMI-ibdMxsXT"
   },
   "outputs": [],
   "source": [
    "#Save the dataframe to csv document for regression model\n",
    "#df_pd = result.toPandas()\n",
    "#df_pd.to_csv(\"/content/drive/Shared drives/718_project/dataset/listing_clean_new(used_for_random_forest_and_GBT).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAp33APpnAgk"
   },
   "source": [
    "## Outlier Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohF0U9faAGLd"
   },
   "source": [
    "### Distribution inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcQ_Qvv_APPj"
   },
   "source": [
    "#### boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8B8dTo1Q57gb"
   },
   "source": [
    "- In the big data environment, extracting data from the distributed file system is infeasible. To se the distribution of each feature, we will calculate the descriptive information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G0ODQLXt6Q8F",
    "outputId": "0980bcc4-3872-4a84-8fb6-bfee8fc8a874"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_distribution(result):\n",
    "  non_continuous_columns = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                            'is_location_exact', 'guests_included', 'instant_bookable',\n",
    "                            'latitude', 'longitude']\n",
    "\n",
    "  numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "  numerical_columns = [col_name for col_name in numerical_columns if col_name not in non_continuous_columns]\n",
    "\n",
    "  stats = list()\n",
    "  for col_name in numerical_columns:\n",
    "    percentiles = result.select(col_name).agg(\n",
    "            fn.expr(f'percentile({col_name}, array(0))').alias('0%'),\n",
    "            fn.expr(f'percentile({col_name}, array(0.25))').alias('25%'), \n",
    "            fn.expr(f'percentile({col_name}, array(0.5))').alias('50%'), \n",
    "            fn.expr(f'percentile({col_name}, array(0.75))').alias('75%'),\n",
    "            fn.expr(f'percentile({col_name}, array(1))').alias('100%')).toPandas()\n",
    "            \n",
    "    _0 = percentiles.loc[0, '0%']\n",
    "    _25 = percentiles.loc[0, '25%']\n",
    "    _50 = percentiles.loc[0, '50%']\n",
    "    _75 = percentiles.loc[0, '75%']\n",
    "    _100 = percentiles.loc[0, '100%']\n",
    "    stats.append({'label': col_name, 'med': _50, 'q1': _25, 'q3': _75, 'whislo': _0, 'whishi': _100})\n",
    "\n",
    "  fig, ax = plt.subplots(len(numerical_columns), 1, sharex=False, sharey='row', figsize=(15, 50))\n",
    "\n",
    "  for idx, s in enumerate(stats):\n",
    "    ax[idx].bxp([stats[idx]], vert=False, showfliers=False);\n",
    "\n",
    "  plt.tight_layout()\n",
    "\n",
    "  display()\n",
    "\n",
    "\n",
    "plot_feature_distribution(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3KTV7i6l8u9"
   },
   "source": [
    "- Some features are not normal distributed in the data space. Convert them by taking logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "l1VPTSwV6QXT",
    "outputId": "8fa4aa52-72ee-4740-cd77-04e2f1b5e4f8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class LogarithmImputer(Transformer):\n",
    "  def __init__(self):\n",
    "    self._excluded_columns = ['id', 'host_id', 'host_is_superhost', 'host_has_profile_pic',\n",
    "                              'host_identity_verified', 'is_location_exact',\n",
    "                              'guests_included', 'instant_bookable', \n",
    "                              'latitude', 'longitude', 'availability_30', \n",
    "                              'availability_60', 'availability_90', 'availability_365', \n",
    "                              ]\n",
    "    self._transofrmed_columns = list()\n",
    "    self._log = fn.udf(lambda x: math.log(x+1), t.DoubleType())\n",
    "\n",
    "  @property\n",
    "  def transofrmed_columns(self):\n",
    "    return self._transofrmed_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "    numerical_columns = [col_name for col_name in numerical_columns if col_name not in self._excluded_columns]\n",
    "    self._transofrmed_columns = numerical_columns\n",
    "\n",
    "    for col_name in self._transofrmed_columns:\n",
    "\n",
    "      result = result.withColumn(col_name, self._log(col_name))\n",
    "\n",
    "    return result\n",
    "\n",
    "logarithm_imputer = LogarithmImputer()\n",
    "logarithm_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer\n",
    "                            , logarithm_imputer])\n",
    "\n",
    "result = logarithm_pipeline.fit(df).transform(df)\n",
    "\n",
    "plot_feature_distribution(result.select(logarithm_imputer.transofrmed_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARoZyCNWnilK"
   },
   "source": [
    "### Winsorizing the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djLz-ALMAcfk"
   },
   "source": [
    "- host id is a categorical feature, it should be revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8xZQyP1nDDZ"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class WinsorizingImputer(Transformer):\n",
    "  def __init__(self):\n",
    "    self._pending_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "    self._excuded_columns = ['id', 'host_id', 'host_is_superhost', 'longitude', 'latitude', 'is_location_exact',\n",
    "                             'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification',\n",
    "                             'price' ]\n",
    "    def winsorize(tail, head):\n",
    "      def _winsorize(value):\n",
    "        if value < tail:\n",
    "          return float(tail)\n",
    "        if value > head:\n",
    "          return float(head)\n",
    "        return float(value)\n",
    "      return _winsorize\n",
    "    self._winsorize = lambda col, tail, head: fn.udf(winsorize(tail, head), t.DoubleType())(col)\n",
    "\n",
    "  @property\n",
    "  def pending_columns(self):\n",
    "    return self._pending_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    self._pending_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "    self._pending_columns = [col_name for col_name in self._pending_columns if col_name not in self._excuded_columns]\n",
    "\n",
    "    for col_name in self._pending_columns:\n",
    "      percentiles = result.agg(\n",
    "          fn.expr(f'percentile({col_name}, array(0.25))').alias('tail'), \n",
    "          fn.expr(f'percentile({col_name}, array(0.75))').alias('head')).toPandas()\n",
    "      tail = percentiles.loc[0, 'tail'][0]\n",
    "      head = percentiles.loc[0, 'head'][0]\n",
    "      result = result.withColumn(col_name, self._winsorize(col_name, tail, head))\n",
    "    result = result.drop(*percentiles.columns)\n",
    "\n",
    "    return result\n",
    "\n",
    "winsorizing_imputer = WinsorizingImputer()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer\n",
    "                            , logarithm_imputer\n",
    "                            , winsorizing_imputer])\n",
    "result = wrangling_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUETRctEdrGY"
   },
   "source": [
    "### Low Variance Features Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2z5Ww74-dt0m",
    "outputId": "927a2443-f7ab-484b-83d7-b5e3d34f35b7"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import feature\n",
    "\n",
    "\n",
    "@singleton\n",
    "class LowVarianceCleaner(Transformer):\n",
    "  def __init__(self):\n",
    "    self._reg_exp = re.compile(\"variance\\((.*)\\)\", re.IGNORECASE)\n",
    "    self._pending_columns = list()\n",
    "    self._excuded_columns = ['id', 'host_id', 'host_is_superhost', 'longitude', 'latitude', 'is_location_exact',\n",
    "                             'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "    self._extract_udf = lambda idx: fn.udf(lambda centered_features: float(centered_features[idx]), t.DoubleType())('centered_features')\n",
    "    self._threshold = 0.01\n",
    "    self._low_variance_columns = list()\n",
    "\n",
    "  @property\n",
    "  def pending_columns(self):\n",
    "    return self._pending_columns\n",
    "\n",
    "  @property\n",
    "  def low_variance_columns(self):\n",
    "    return self._low_variance_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "\n",
    "    self._pending_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n",
    "    self._pending_columns = [col_name for col_name in self._pending_columns if col_name not in self._excuded_columns]\n",
    "\n",
    "    standardized_result = Pipeline(stages=[feature.VectorAssembler(inputCols=self._pending_columns, outputCol='features'),\n",
    "                                            feature.StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='centered_features')]) \\\n",
    "          .fit(result).transform(result)\n",
    "\n",
    "    for idx, col_name in enumerate(self._pending_columns):\n",
    "      standardized_result = standardized_result.withColumn(col_name, self._extract_udf(idx))\n",
    "    standardized_result = standardized_result.drop('centered_features')\n",
    "\n",
    "    numeric_variances = standardized_result.agg({col_name : 'variance' for col_name in self._pending_columns})\n",
    "    numeric_variances = numeric_variances.select(*[fn.col(col).alias(self._reg_exp.search(col).group(1)) for col in numeric_variances.columns]) \\\n",
    "      .toPandas().T.iloc[:,0]\n",
    "    numeric_variances = numeric_variances[(numeric_variances < self._threshold) | (numeric_variances.isna())]\n",
    "\n",
    "    self._low_variance_columns = numeric_variances.index\n",
    "\n",
    "    result = result.drop(*self._low_variance_columns)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "low_variance_cleaner = LowVarianceCleaner()\n",
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer\n",
    "                            , logarithm_imputer\n",
    "                            , winsorizing_imputer\n",
    "                            , low_variance_cleaner])\n",
    "\n",
    "result = wrangling_pipeline.fit(df).transform(df)\n",
    "\n",
    "print(shape(result))\n",
    "\n",
    "#return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuH3nOkIzPf8"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TR2ahfWmKlhJ"
   },
   "source": [
    "### Tree-based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "of4IBzWAKkkx"
   },
   "outputs": [],
   "source": [
    "wrangling_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , missing_value_imputer\n",
    "                            , low_variance_cleaner])\n",
    "\n",
    "result = wrangling_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5o6pzyBI907"
   },
   "source": [
    "#### Random Forest\n",
    "- Explore feature importance By using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JjFOpX_G_QF"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xDyGLYpHB2R"
   },
   "outputs": [],
   "source": [
    "result = spark.read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"multiLine\", \"true\")\\\n",
    "        .option('inferSchema', 'true')\\\n",
    "        .option('escape', '\"')\\\n",
    "        .csv(get_training_filename('listing_clean_new_used_for_random_forest_and_GBT.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojhf_DMkfC14"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,CountVectorizer,IDF,StringIndexer,OneHotEncoder\n",
    "categorical_cols_new = ['neighbourhood_group_cleansed','host_response_time',\n",
    "       'property_type', 'room_type', 'bed_type','cancellation_policy']\n",
    "col_idx_new=['neighbourhood_group_cleansed_IDX',\n",
    " 'host_response_time_IDX',\n",
    " 'property_type_IDX',   \n",
    " 'room_type_IDX',\n",
    " 'bed_type_IDX',\n",
    " 'cancellation_policy_IDX']\n",
    "\n",
    "indexers_new = [StringIndexer(inputCol=col, outputCol = col + \"_IDX\")\\\n",
    "            .setHandleInvalid(\"keep\") for col in categorical_cols_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZ37loarfUHQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "col_numeric = [c for c in result.columns if ( c not in categorical_cols_new\n",
    "                                             and c not in ['host_verifications','amenities','price'])]\n",
    "assemble1 = VectorAssembler(inputCols= col_numeric + col_idx_new ,outputCol='features')\n",
    "reg = RandomForestRegressor(labelCol='price',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2Qz7eE8hag6"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "transformer1 = Pipeline(stages=indexers_new  + [assemble1,reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fzdc4u1hxju"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "df_train,df_test = result.randomSplit([0.7,0.3],seed=42)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(reg.numTrees, [int(x) for x in np.linspace(start = 3, stop = 15, num = 3)]) \\\n",
    "    .addGrid(reg.maxDepth, [int(x) for x in np.linspace(start = 3, stop = 15, num = 3)]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='price') \n",
    "crossval = CrossValidator(estimator=transformer1,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0841SJFibPL"
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXi_X_7ejo_Y"
   },
   "outputs": [],
   "source": [
    "bestPipeline = cvModel.bestModel\n",
    "bestModel = bestPipeline.stages[-1]\n",
    "importances = bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-eQRvtwFjyAQ"
   },
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame(importances.toArray(), columns=['importances'])\n",
    "fi_df['feature'] = pd.Series(col_numeric+col_idx_new)\n",
    "fi_df.sort_values(by=['importances'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "-46whMQMj_3v",
    "outputId": "f4a68776-8b13-4372-9ae7-788853d1c446"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "fi_df.plot.barh(x='feature', \n",
    "               y ='importances',\n",
    "               figsize=(20,8), \n",
    "               title='Feature Importances', \n",
    "               fontsize=10)\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JX315UO7j5ne"
   },
   "source": [
    "**Run random forest regression By filtering importance which is bigger than 0.03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60_M-H5ilKEy"
   },
   "outputs": [],
   "source": [
    "fi_df_new = fi_df[fi_df['importances']>0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "37ierHbBSV_x",
    "outputId": "c5a2c2ec-c6eb-4b74-984e-cc48a28d597c"
   },
   "outputs": [],
   "source": [
    "fi_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV_UsvgjlfDQ"
   },
   "outputs": [],
   "source": [
    "num_feature_filtered = []\n",
    "for i in fi_df_new['feature'].tolist():\n",
    "    if i not in ['neighbourhood_group_cleansed_IDX','cancellation_policy_IDX']:\n",
    "       num_feature_filtered.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "QVimHdfhT7YG",
    "outputId": "12a8c7e6-7abf-47be-8c75-049096a5143b"
   },
   "outputs": [],
   "source": [
    "num_feature_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRHb3DwYhCCI"
   },
   "outputs": [],
   "source": [
    "# change categorial col to Index\n",
    "categorical_cols_filtered = ['neighbourhood_group_cleansed'\n",
    "                            ,'cancellation_policy']\n",
    "indexers_filtered = [StringIndexer(inputCol=col, outputCol = col + \"_IDX\")\\\n",
    "            .setHandleInvalid(\"keep\") for col in categorical_cols_filtered]\n",
    "\n",
    "encoded_filtered = [OneHotEncoder(inputCol = col + \"_IDX\", outputCol = col + '_Vec') for col in categorical_cols_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiF8gDJwyBOo"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "cv1 = CountVectorizer()\\\n",
    "    .setInputCol('host_verifications')\\\n",
    "    .setOutputCol('tf1')\n",
    "\n",
    "cv2 = CountVectorizer()\\\n",
    "    .setInputCol('amenities')\\\n",
    "    .setOutputCol('tf2')\n",
    "\n",
    "idf1 = IDF().\\\n",
    "    setInputCol(\"tf1\").\\\n",
    "    setOutputCol(\"tfidf1\")\n",
    "\n",
    "idf2 = IDF().\\\n",
    "    setInputCol(\"tf2\").\\\n",
    "    setOutputCol(\"tfidf2\")\n",
    "\n",
    "assemble1 = VectorAssembler(inputCols= num_feature_filtered, outputCol='features')\n",
    "\n",
    "assemble2 = VectorAssembler(inputCols= ['features','tfidf1','tfidf2','neighbourhood_group_cleansed_Vec'\n",
    "                                        ,'cancellation_policy_Vec'], outputCol='final_features')\n",
    "\n",
    "reg = RandomForestRegressor(labelCol='price',featuresCol='final_features')\n",
    "\n",
    "transformer_final = Pipeline(stages=indexers_filtered + encoded_filtered + [cv1,idf1,cv2,idf2\n",
    "                                                                            ,assemble1,assemble2,reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LS-j2n7rosdS"
   },
   "outputs": [],
   "source": [
    "#just run one time\n",
    "from pyspark.sql.functions import array\n",
    "result =result.withColumn('host_verifications', array(result['host_verifications']))\n",
    "result =result.withColumn('amenities', array(result['amenities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1_BPI3smrRK"
   },
   "outputs": [],
   "source": [
    "df_train,df_test = result.randomSplit([0.7,0.3],seed=42)\n",
    "paramGrid_1 = ParamGridBuilder() \\\n",
    "    .addGrid(reg.numTrees, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n",
    "    .addGrid(reg.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator_1 = RegressionEvaluator(labelCol='price') \n",
    "crossval_1 = CrossValidator(estimator=transformer_final,\n",
    "                          estimatorParamMaps=paramGrid_1,\n",
    "                          evaluator=evaluator_1,\n",
    "                          numFolds=3)\n",
    "cvModel_1 = crossval_1.fit(df_train)\n",
    "bestModel_1 = cvModel_1.bestModel\n",
    "preds_1 = bestModel_1.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BWVjKcfepPPv",
    "outputId": "deadb560-8c43-49f0-ef03-75d508e9350e"
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(preds_1, {evaluator.metricName: 'rmse'})\n",
    "r2 = evaluator.evaluate(preds_1, {evaluator.metricName: 'r2'})\n",
    "print(' RMSE: ' + str(rmse))\n",
    "print(' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "jxf5aVTC-x-s",
    "outputId": "f232cefd-6136-4aec-9281-412d6fc6e842"
   },
   "outputs": [],
   "source": [
    "rfResult = preds_1.toPandas()\n",
    "\n",
    "plt.plot(rfResult.price, rfResult.prediction, 'bo')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Prediction')\n",
    "plt.suptitle(\"Model Performance RMSE: %f\" % rmse)\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RbGFjxpsKRFo",
    "outputId": "5d07cb70-44cc-4c5b-c035-308d6d2603dd"
   },
   "outputs": [],
   "source": [
    "print('numTrees - ', bestModel_1.stages[-1].getNumTrees)\n",
    "print('maxDepth - ', bestModel_1.stages[-1].getOrDefault('maxDepth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiEf5Esl0ef"
   },
   "source": [
    "#### Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Q0F6fxwl6MH"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "\n",
    "gbt = GBTRegressor(labelCol='price',featuresCol='final_features')\n",
    "\n",
    "transformer_final_gbt = Pipeline(stages=indexers_filtered + encoded_filtered + [cv1,idf1,cv2,idf2\n",
    "                                                                            ,assemble1,assemble2,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We trained 70% of total data and test 30% of total data, but it cost 40 minutes by using GBT regression.\n",
    "# If it run too much time, databrick would stop, so I sample our data. Then everything can go smoothly in databricks\n",
    "result_sample = result.sample(False,0.2,42)\n",
    "df_train,df_test = result_sample.randomSplit([0.7,0.3],seed=42)\n",
    "paramGrid_2 = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n",
    "    .addGrid(gbt.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator_2 = RegressionEvaluator(labelCol='price') \n",
    "crossval_2 = CrossValidator(estimator=transformer_final_gbt,\n",
    "                          estimatorParamMaps=paramGrid_2,\n",
    "                          evaluator=evaluator_2,\n",
    "                          numFolds=3)\n",
    "cvModel_2 = crossval_2.fit(df_train)\n",
    "bestModel_2 = cvModel_2.bestModel\n",
    "preds_2 = bestModel_2.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JIhFYspxmB49",
    "outputId": "d3b3bc2f-2b2e-4836-b9b3-61d6369fe86a"
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(preds_2, {evaluator.metricName: 'rmse'})\n",
    "r2 = evaluator.evaluate(preds_2, {evaluator.metricName: 'r2'})\n",
    "print(' RMSE: ' + str(rmse))\n",
    "print(' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "mhTotZ3DmDts",
    "outputId": "e8a018ad-8161-4315-cc5c-185592bb410f"
   },
   "outputs": [],
   "source": [
    "rfResult = preds_2.toPandas()\n",
    "\n",
    "plt.plot(rfResult.price, rfResult.prediction, 'bo')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Prediction')\n",
    "plt.suptitle(\"GBT Model Performance RMSE: %f\" % rmse)\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "w1S8bIZPKX3t",
    "outputId": "9a036b29-f396-493b-ac33-c2d164d661e7"
   },
   "outputs": [],
   "source": [
    "print('numIter - ', bestModel_2.stages[-1].getOrDefault('maxIter'))\n",
    "print('maxDepth - ', bestModel_2.stages[-1].getOrDefault('maxDepth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSFsTj9tzNyx"
   },
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoAw5YD2zprP"
   },
   "source": [
    "- Features reduce quickly in the lasso model. For a dataset with high dimensional space, it is hard to find a close relationshi among observations. Under the L1 penalty term regularization, the lasso model will filter the most relevant features. \n",
    "- In the elastic net regression, the model becomes lasso regression when α equals to 1, which takes the L1 penalty term only. \n",
    "- The scree plot direct us the reasonable λ to apply on the penaly term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D7YZvcPC0ia0",
    "outputId": "a53cf90e-e7ea-4417-ad64-6ea1472ec988"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import regression, tuning, evaluation\n",
    "\n",
    "\n",
    "def build_index_transformer(columns):\n",
    "  return [StringIndexer(inputCol=col, outputCol = col + \"_IDX\", handleInvalid=\"keep\")  for col in columns]\n",
    "\n",
    "categorical_type_features = ['property_type', 'room_type', 'bed_type']\n",
    "indexers = build_index_transformer([col_name for col_name in categorical_type_features if col_name in df.columns])\n",
    "categorical_type_features_remover = ColumnRemover(categorical_type_features)\n",
    "non_numerical_column_remover = ColumnRemover(['id', 'host_id', 'host_since', 'first_review', 'last_review', 'host_response_time',\n",
    "                                              'host_verifications', 'neighbourhood_group_cleansed',  'amenities', 'cancellation_policy', \n",
    "                                              'latitude', 'longitude'])\n",
    "\n",
    "lasso_dataset_generate_pipeline = Pipeline(stages=[bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , non_numerical_column_remover\n",
    "                            , missing_value_imputer\n",
    "                            , logarithm_imputer\n",
    "                            , winsorizing_imputer\n",
    "                            , low_variance_cleaner\n",
    "                            , *indexers\n",
    "                            , categorical_type_features_remover])\n",
    "\n",
    "\n",
    "result = lasso_dataset_generate_pipeline.fit(df).transform(df)\n",
    "\n",
    "print(f'Lasso dataset shape: {shape(result)}')\n",
    "\n",
    "train_data, test_data = split_data(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_HakzpgSFzu"
   },
   "source": [
    "- Training Lasso Model and visualize the performance via scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0I6ohtJRSCQB"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "_lambda = pd.Series(np.arange(0.1,0.34,0.02)).tolist()\n",
    "\n",
    "_train_r2 = list()\n",
    "_test_r2 = list()\n",
    "_coef = list()\n",
    "def search_lambda(train_data, lbds, train_r2, test_r2, coef):\n",
    "  assembler = feature.VectorAssembler(inputCols=train_data.drop('price').columns, outputCol='features')\n",
    "  standardizer = feature.StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "  normalizer = feature.Normalizer(inputCol=\"scaled_features\", outputCol=\"norm_features\", p=2.0)\n",
    "\n",
    "  lr = regression.LinearRegression() \\\n",
    "    .setLabelCol('price') \\\n",
    "    .setFeaturesCol('norm_features') \\\n",
    "    .setMaxIter(10) \\\n",
    "    .setElasticNetParam(1)\n",
    "\n",
    "  lasso_cleaning_pipeline = Pipeline(stages=[assembler, standardizer, normalizer, lr])\n",
    "\n",
    "  evaluator = evaluation.RegressionEvaluator() \\\n",
    "    .setLabelCol(lasso_cleaning_pipeline.getStages()[-1].getLabelCol()) \\\n",
    "    .setMetricName('r2')\n",
    "\n",
    "\n",
    "\n",
    "  for lbd in lbds:\n",
    "    paramGrid = tuning.ParamGridBuilder()\\\n",
    "        .addGrid(lr.regParam, [lbd]) \\\n",
    "        .build()\n",
    "\n",
    "    tvs = tuning.TrainValidationSplit(estimator=lasso_cleaning_pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              trainRatio=0.7)\n",
    "\n",
    "\n",
    "    model = tvs.fit(train_data)\n",
    "\n",
    "    train_r2.append(evaluator.evaluate(model.transform(train_data)))\n",
    "    test_r2.append(evaluator.evaluate(model.transform(test_data)))\n",
    "    coef.append(model.bestModel.stages[-1].coefficients.toArray())\n",
    "\n",
    "search_lambda(train_data, _lambda, _train_r2, _test_r2, _coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rm2TWyJznbUU"
   },
   "source": [
    "- Visualize performances of each lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "colab_type": "code",
    "id": "y9CbCwpGnrXE",
    "outputId": "a04aedaa-e5d6-454e-b6f2-0a52e3d9d11d"
   },
   "outputs": [],
   "source": [
    "def polt_model_performance(lbds, train_coef, test_coef, title_xys_1, title_xys_2):\n",
    "  plt.clf()\n",
    "  fig, ax = plt.subplots(2, 1, figsize=(20, 7))\n",
    "  ax[0].plot(lbds, train_coef, 'o-', linewidth=2, label=\"Training Set\")\n",
    "  ax[0].plot(lbds, test_coef, 'o-', linewidth=2, label=\"Test Set\")\n",
    "  ax[0].set_title(title_xys_1[0], fontSize=25)\n",
    "  ax[0].set_xlabel(title_xys_1[1])\n",
    "  ax[0].set_ylabel(title_xys_1[2])\n",
    "  ax[1].plot(lbds, [pair[0] - pair[1] for pair in zip(train_coef, test_coef)], 'o-', linewidth=2, label=f\"{title_xys_2[2]}\")\n",
    "  ax[1].set_title(title_xys_2[0], fontSize=25)\n",
    "  ax[1].set_xlabel(title_xys_2[1])\n",
    "  ax[1].set_ylabel(title_xys_2[2])\n",
    "  ax[0].legend()\n",
    "  ax[1].legend()\n",
    "\n",
    "  plt.tight_layout()\n",
    "  display()\n",
    "\n",
    "polt_model_performance(_lambda, _train_r2, _test_r2, \n",
    "                       ('Model Performance of Lasso Regression', 'Lambda', 'R^2'),\n",
    "                       ('Difference of R^2 Between Training Set and Test Set', 'Lambda', 'Difference of R^2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CCxW_wcAOSw"
   },
   "source": [
    "- Trending of the weight for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aB_Dg6xcANhM",
    "outputId": "2eea9b1d-9e89-47aa-c767-0a6bbb1c040f"
   },
   "outputs": [],
   "source": [
    "\n",
    "__coef = np.array(_coef)\n",
    "\n",
    "def plot_importance(column_names, lbds, coef_matrix, threshold, title, x_lab, y_lab):\n",
    "  plt.clf()\n",
    "  plt.figure(figsize=(20, 17))\n",
    "  for idx, col_name in enumerate(column_names):\n",
    "    plt.plot(lbds, coef_matrix[:,idx], 'o-', linewidth=2, label=col_name)\n",
    "    c = coef_matrix[0,idx]\n",
    "    if abs(c) > threshold:\n",
    "      plt.annotate(col_name, (lbds[4], coef_matrix[4,idx]))\n",
    "\n",
    "  plt.title(title, fontSize=25)\n",
    "  plt.xlabel(x_lab)\n",
    "  plt.ylabel(y_lab)\n",
    "\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.tight_layout()\n",
    "  display()\n",
    "\n",
    "plot_importance(train_data.drop('price').columns, _lambda, __coef, 0.25, 'Weight change on each feature', 'Lambda', 'Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSQTX7J5HOrO"
   },
   "source": [
    "- According to the scree plot, nearly all weights of features have been penalized to zero. To get a better insight of the trend of features, we will narrow down the lambda search range and run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4D9I0LXHS0c"
   },
   "outputs": [],
   "source": [
    "_lambda = pd.Series(np.arange(-10.5,-3,1)).apply(math.exp).sort_values().tolist()\n",
    "_train_r2 = list()\n",
    "_test_r2 = list()\n",
    "_coef = list()\n",
    "\n",
    "search_lambda(train_data, _lambda, _train_r2, _test_r2, _coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TIrRJCAnInfB",
    "outputId": "c5c1fcb6-4812-499b-f84e-9a250e88c397"
   },
   "outputs": [],
   "source": [
    "__coef = np.array(_coef)\n",
    "\n",
    "plot_importance(train_data.drop('price').columns, _lambda, __coef, 0.4, 'Weight change on each feature', 'Lambda', 'Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "betHGgdSHeuh"
   },
   "source": [
    "- Set 0.25 as the threshold. When lambda is near 0.01, the features with absolute weight above this number when will be the important features of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "MOZzq5VUSQN-",
    "outputId": "b1f4eeb2-a28c-4666-a730-ef99e72f1d7f"
   },
   "outputs": [],
   "source": [
    "idx = len(_lambda) - len([lbd for lbd in _lambda if lbd > 0.01])\n",
    "\n",
    "lass_weight = pd.DataFrame({'weight': __coef[idx,:], 'abs': np.abs(__coef[idx,:])}, \n",
    "                           index=train_data.drop('price').columns)\n",
    "lass_weight = lass_weight.sort_values(by='abs', ascending=False)\n",
    "\n",
    "feasible_feature = lass_weight[lass_weight['abs'] > 0.25].index.values.tolist()\n",
    "feasible_feature = [col_name.replace('_IDX', '') for col_name in feasible_feature]\n",
    "print(*feasible_feature, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4kay0utHoaC"
   },
   "source": [
    "The model fits the data space pretty well, so the regression coefficient is credible to filter out the useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRGSKx1R4kqj"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-I9FoaQF4n69"
   },
   "source": [
    "## Data Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVuTr0WAh5_a"
   },
   "source": [
    "## Elastic Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPhuewVHXiFC"
   },
   "source": [
    "- Build the dataset with useful features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Upzo540UXgh1",
    "outputId": "04cd074c-2335-4d12-f423-3d5f6fb0e884"
   },
   "outputs": [],
   "source": [
    "@singleton\n",
    "class ColumnSelector(Transformer):\n",
    "\n",
    "  def __init__(self, selected_columns):\n",
    "    self._selected_columns = selected_columns + ['id', 'price']\n",
    "\n",
    "  @property\n",
    "  def selected_columns(self):\n",
    "    return self._selected_columns\n",
    "\n",
    "  def _transform(self, df: DataFrame) -> DataFrame:\n",
    "    result = df\n",
    "    result = result.select(self._selected_columns)\n",
    "    return result\n",
    "\n",
    "df_lasso = load_data('listings.csv')\n",
    "\n",
    "column_selector = ColumnSelector(feasible_feature)\n",
    "indexers = build_index_transformer([col_name for col_name in categorical_type_features if col_name in feasible_feature])\n",
    "\n",
    "\n",
    "elasticnet_pipeline = Pipeline(stages=[column_selector\n",
    "                            , bool_converter\n",
    "                            , currency_converter\n",
    "                            , rate_converter\n",
    "                            , zero_variance_cleaner\n",
    "                            , literature_column_remover\n",
    "                            , url_column_remover\n",
    "                            , incomplete_column_remover\n",
    "                            , sentement_analysis_column_remover\n",
    "                            , missing_review_record_remover\n",
    "                            , non_numerical_column_remover\n",
    "                            , missing_value_imputer\n",
    "                            , logarithm_imputer\n",
    "                            , winsorizing_imputer\n",
    "                            , low_variance_cleaner\n",
    "                            , *indexers\n",
    "                            , categorical_type_features_remover\n",
    "                            ])\n",
    "\n",
    "\n",
    "result = elasticnet_pipeline.fit(df_lasso).transform(df_lasso)\n",
    "\n",
    "print(shape(result))\n",
    "\n",
    "train_data, test_data = split_data(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "zVH6JJFhuEJj",
    "outputId": "92e7853f-1d35-4f0d-dea9-f7a853b4a70a"
   },
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq1EnhRsCcJ8"
   },
   "outputs": [],
   "source": [
    "_elastic_net_train_r2 = list()\n",
    "_elastic_net_test_r2 = list()\n",
    "_elastic_net_coef = list()\n",
    "\n",
    "_elastic_net_alpha = np.arange(0,1,.3)\n",
    "_elastic_net_lambda = np.arange(.05,.95,.15)\n",
    "\n",
    "def search_lambda_alpha(train_data, test_data, lbds, alphas, train_r2, test_r2, coef):\n",
    "  assembler = feature.VectorAssembler(inputCols=train_data.drop('price').columns, outputCol='features')\n",
    "  standardizer = feature.StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "  normalizer = feature.Normalizer(inputCol=\"scaled_features\", outputCol=\"norm_features\", p=2.0)\n",
    "\n",
    "  lr = regression.LinearRegression() \\\n",
    "    .setLabelCol('price') \\\n",
    "    .setFeaturesCol('norm_features') \\\n",
    "    .setMaxIter(10) \\\n",
    "    .setElasticNetParam(1)\n",
    "\n",
    "  lasso_cleaning_pipeline = Pipeline(stages=[assembler, standardizer, normalizer, lr])\n",
    "\n",
    "  evaluator = evaluation.RegressionEvaluator() \\\n",
    "    .setLabelCol(lasso_cleaning_pipeline.getStages()[-1].getLabelCol()) \\\n",
    "    .setMetricName('r2')\n",
    "\n",
    "\n",
    "\n",
    "  # print(*train_data.columns, sep='\\n')\n",
    "  for a in alphas:\n",
    "    for lbd in lbds:\n",
    "      paramGrid = tuning.ParamGridBuilder()\\\n",
    "          .addGrid(lr.regParam, [lbd]) \\\n",
    "          .addGrid(lr.elasticNetParam, [a]) \\\n",
    "          .build()\n",
    "\n",
    "      tvs = tuning.TrainValidationSplit(estimator=lasso_cleaning_pipeline,\n",
    "                                estimatorParamMaps=paramGrid,\n",
    "                                evaluator=evaluator,\n",
    "                                trainRatio=0.7)\n",
    "\n",
    "      model = tvs.fit(train_data)\n",
    "\n",
    "      train_r2.append(evaluator.evaluate(model.transform(train_data)))\n",
    "      test_r2.append(evaluator.evaluate(model.transform(test_data)))\n",
    "      coef.append(model.bestModel.stages[-1].coefficients.toArray())\n",
    "\n",
    "search_lambda_alpha(train_data, test_data,\n",
    "                    _elastic_net_lambda, _elastic_net_alpha,\n",
    "                    _elastic_net_train_r2, _elastic_net_test_r2, _elastic_net_coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCVGWzDodQU_"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(20, 17))\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 7))\n",
    "for idx, a in enumerate(_elastic_net_alpha):\n",
    "    coef_idx = len(_elastic_net_lambda) * idx\n",
    "    ax[0].plot(_elastic_net_lambda, _elastic_net_train_r2[coef_idx:coef_idx+len(_elastic_net_lambda)],\n",
    "               'o-', linewidth=2, label=f\"Training Set-Alpha: {a}\")\n",
    "    ax[0].plot(_elastic_net_lambda, _elastic_net_test_r2[coef_idx:coef_idx+len(_elastic_net_lambda)],\n",
    "               'o-', linewidth=2, label=f\"Test Set-Alpha: {a}\")\n",
    "    ax[0].set_title('Elastic Net Regression Performance', fontSize=25)\n",
    "    ax[0].set_xlabel('R^2')\n",
    "    ax[0].set_ylabel('lambda')\n",
    "    diff = [pair[0] - pair[1] for pair in zip(\n",
    "        _elastic_net_train_r2[coef_idx:coef_idx+len(_elastic_net_lambda)], \n",
    "        _elastic_net_test_r2[coef_idx:coef_idx+len(_elastic_net_lambda)])]\n",
    "    ax[1].plot(_elastic_net_lambda, diff, 'o-', linewidth=2, label=f\"Difference of R^2-Alpha: {a}\")\n",
    "    ax[1].set_title('Difference of the Elastic Net Regression Preference', fontSize=25)\n",
    "    ax[1].set_xlabel('lambda')\n",
    "    ax[1].set_ylabel('Difference of R^2')\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcOKaUusitv6"
   },
   "source": [
    "#### Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebIiV7-EjQZV"
   },
   "outputs": [],
   "source": [
    "reg_coef = pd.DataFrame(_elastic_net_coef, columns=[col_name for col_name in column_selector.selected_columns if col_name not in ['price', 'id']])\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "\n",
    "sns.boxplot(data=reg_coef.abs(), orient='h')\n",
    "\n",
    "plt.title(f\"Distribution of Feature Regression Coefficient\", fontsize=30)\n",
    "plt.ylabel('Feature Name', fontsize=18)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Regression Coefficient', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "display()\n",
    "\n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Hi_aoMcdSgY"
   },
   "source": [
    "#### Insights\n",
    "\n",
    "1. Rooms clearance indicates the class of a house. Airbnb can dispatch vouchers to cover miscellaneous fee, like cleanning fee to increase retension rate.\n",
    "2. A flexible arrangement is a big determinent. Guest may pay extra price to buy the offer such as booking travel protection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diAIU3o4hy3f"
   },
   "source": [
    "## Natrual Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEjRZHiTdJbB"
   },
   "source": [
    "### Term Frequency in Host Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4-E_9n5iSlk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_df = load_data('listings.csv')\n",
    "nlp_df1=nlp_df.select(\"name\",\"review_scores_rating\",\"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZR321XqlKLb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">root\n",
       "-- name: string (nullable = true)\n",
       "-- review_scores_rating: integer (nullable = true)\n",
       "-- description: string (nullable = true)\n",
       "\n",
       "Out[15]: (9023, 3)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_df1.printSchema()\n",
    "nlp_df1.count(),len(nlp_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "nlp_df1 = nlp_df1.withColumn(\"review_scores_rating\", nlp_df1[\"review_scores_rating\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTtLzQCJlS3p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">root\n",
       "-- name: string (nullable = true)\n",
       "-- review_scores_rating: integer (nullable = true)\n",
       "-- description: string (nullable = true)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_df2=nlp_df1.where(nlp_df1.review_scores_rating == 100)\n",
    "nlp_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[18]: (1748, 3)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_df2.count(),len(nlp_df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "def language_detection(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "language_udf = udf(language_detection)\n",
    "nlp_df2 = nlp_df2.withColumn('language',language_udf(nlp_df2['description']))\n",
    "nlp_df2= nlp_df2.filter(nlp_df2['language']=='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYkxQJkPm2rG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "tokenizer = RegexTokenizer(minTokenLength=3).setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"description\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajx2Na6xmUuL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3v9kXN1nGlT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_pipeline = Pipeline(stages=[tokenizer, sw_filter]).fit(nlp_df2)\n",
    "nlp_df3 = nlp_pipeline.transform(nlp_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7qjHcLvnOiS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer().setInputCol('filtered_d').setOutputCol(\"tf\")\n",
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idM39SDKnQju"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_pipeline=Pipeline(stages=[cv,idf]).fit(nlp_df3)\n",
    "nlp_df4 = tfidf_pipeline.transform(nlp_df3)\n",
    "\n",
    "words = tfidf_pipeline.stages[0].vocabulary\n",
    "IDF_values = tfidf_pipeline.stages[1].idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBSTHr9PnbaN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "voca_idf = pd.DataFrame({'word': words, 'IDF': IDF_values})\n",
    "highest_idf=voca_idf.sort_values('IDF',ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMvfhS9dnesJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7405</th>\n",
       "      <td>wowed</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>lover</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>intereaction</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>pallet</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5349</th>\n",
       "      <td>eliminate</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5348</th>\n",
       "      <td>slates</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5347</th>\n",
       "      <td>das</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>xiii</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>neighborho</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>cement</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5343</th>\n",
       "      <td>ers</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>vegetation</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5341</th>\n",
       "      <td>ban</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5340</th>\n",
       "      <td>churchill</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5339</th>\n",
       "      <td>nationally</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>magnesume</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5337</th>\n",
       "      <td>gig</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td>lifts</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>vacationloghome</td>\n",
       "      <td>6.764462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highest_idf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloudConvertDF =voca_idf.set_index('word').T.to_dict('records')\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate_from_frequencies(dict(*wordcloudConvertDF))\n",
    "plt.figure(figsize=(14, 10))    \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0DPwYtCofng"
   },
   "source": [
    "### Comments sentiment ananlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NFK7zVBoeEl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_df=df_reviews\n",
    "review_df.toPandas().head()\n",
    "review_df = review_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This cell we randomly extract 4500 comments as sample to analyze instead of whole dataset to decrease the time, we can get the whole data result without running this cell\n",
    "review_df=review_df.sample(False, 0.1, seed=21).limit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ze79xeQxplbk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "def compound_score(text):\n",
    "    compound_value = analyzer.polarity_scores(text)['compound']\n",
    "   \n",
    "    return compound_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkLMTc0EqclQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "score_udf = udf(compound_score,FloatType())\n",
    "review_df = review_df.withColumn('sentiment_compound',score_udf(review_df['comments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "def language_detection(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "      \n",
    "language_udf = udf(language_detection)\n",
    "review_df = review_df.withColumn('language',language_udf(review_df['comments']))\n",
    "review_df = review_df.filter(review_df['language']=='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjDfcuoXrzME"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "tokenizer2 = RegexTokenizer(minTokenLength=3).setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"comments\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw_filter2 = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered_com\")\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv2 = CountVectorizer().setInputCol('filtered_com').setOutputCol(\"tf\")\n",
    "from pyspark.ml.feature import IDF\n",
    "idf2 = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deVg3Je-r9RC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_pipeline2 = Pipeline(stages=[tokenizer2, sw_filter2, cv2, idf2]).fit(review_df)\n",
    "review_df1 = nlp_pipeline2.transform(review_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_61NLkgTsIc0"
   },
   "outputs": [],
   "source": [
    "review_df2 = review_df1.select('listing_id','reviewer_id','filtered_com','comments','sentiment_compound','tf','tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EoMvhMD0sVlK"
   },
   "outputs": [],
   "source": [
    "review=review_df2.select(fn.when(fn.col('sentiment_compound') > 0,1).otherwise(0).alias(\"score\"),\n",
    "                                'listing_id','comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCpKdbQas7OI"
   },
   "outputs": [],
   "source": [
    "review.groupby('score').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewp = review.filter(review['score']=='1')\n",
    "reviewn= review.filter(review['score']=='0')\n",
    "reviewp=reviewp.sample(False, 0.1, seed=0).limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewc = reviewp.union(reviewn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvhSVya8uBzP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "training_df, validation_df, testing_df = reviewc.randomSplit([0.5, 0.3, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ob6wTtghuGim"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('score').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)\n",
    "lr_pipeline = Pipeline(stages=[nlp_pipeline2, lr]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKnSoMeTvc7o"
   },
   "outputs": [],
   "source": [
    "lr_pipeline.transform(validation_df).\\\n",
    "    select(fn.expr('float(prediction = score)').alias('correct')).\\\n",
    "    select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "if1-opclwmxQ"
   },
   "source": [
    "### Sentiment Prediction Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fVR2LvyyO2P"
   },
   "outputs": [],
   "source": [
    "vocabulary = nlp_pipeline2.stages[2].vocabulary\n",
    "weights = lr_pipeline.stages[-1].coefficients.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0p5agCjWxahR"
   },
   "outputs": [],
   "source": [
    "coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\n",
    "coeffs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6jN_e_5yVrz"
   },
   "outputs": [],
   "source": [
    "coeffs_df.sort_values('weight').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH1847VPyYW9"
   },
   "outputs": [],
   "source": [
    "coeffs_df.sort_values('weight', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5xFAMw6yeXa"
   },
   "source": [
    "It seems a lot of noise in this model and the model is overfiiting cuz these words don't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ziXtRvAybZC"
   },
   "outputs": [],
   "source": [
    "lambda_par = 0.02\n",
    "alpha_par = 0.3\n",
    "en_lr = LogisticRegression().\\\n",
    "        setLabelCol('score').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVejOzjDyyJI"
   },
   "outputs": [],
   "source": [
    "en_lr_estimator = Pipeline(\n",
    "    stages=[tokenizer2, sw_filter2, cv2, idf2, en_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhCBdUETy5Hz"
   },
   "outputs": [],
   "source": [
    "en_lr_pipeline = en_lr_estimator.fit(training_df)\n",
    "en_lr_pipeline.transform(validation_df).select(fn.avg(fn.expr('float(prediction = score)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ58sQ3I2o4N"
   },
   "outputs": [],
   "source": [
    "en_weights = en_lr_pipeline.stages[-1].coefficients.toArray()\n",
    "en_coeffs_df = pd.DataFrame({'word': en_lr_pipeline.stages[2].vocabulary, 'weight': en_weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YZqUfrY2rWg"
   },
   "outputs": [],
   "source": [
    "en_coeffs_df.sort_values('weight').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxnwB5wU2uay"
   },
   "outputs": [],
   "source": [
    "en_coeffs_df.sort_values('weight', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oa0gKiOd20p1"
   },
   "outputs": [],
   "source": [
    "en_coeffs_df.query('weight == 0.0').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmKUlLyE249f"
   },
   "outputs": [],
   "source": [
    "en_coeffs_df.query('weight == 0.0').shape[0]/en_coeffs_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ui59ff6S270o"
   },
   "outputs": [],
   "source": [
    "en_coeffs_df.query('weight == 0.0').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mdioI8z2-pW"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "grid = ParamGridBuilder().\\\n",
    "    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n",
    "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvywRxQI3CUP"
   },
   "outputs": [],
   "source": [
    "all_models = []\n",
    "for j in range(len(grid)):\n",
    "    print(\"Fitting model {}\".format(j+1))\n",
    "    model = en_lr_estimator.fit(training_df, grid[j])\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMKcYZlW3FaV"
   },
   "outputs": [],
   "source": [
    "accuracies = [m.\\\n",
    "    transform(validation_df).\\\n",
    "    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n",
    "    first().\\\n",
    "    accuracy for m in all_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CmbNtCn3I5e"
   },
   "outputs": [],
   "source": [
    "best_model_idx = np.argmax(accuracies)\n",
    "print(\"best model index =\", best_model_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IH8CvSN3LqC"
   },
   "outputs": [],
   "source": [
    "grid[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqyeS76I3QgO"
   },
   "outputs": [],
   "source": [
    "best_model = all_models[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvsvqyrU3R7k"
   },
   "outputs": [],
   "source": [
    "best_model.\\\n",
    "    transform(testing_df).\\\n",
    "    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_living.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbbOweilUx6T"
   },
   "source": [
    "## Recommend System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Users (The score of reviewers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system used collaborative filtering to help users get which houses they could live according to their past reviews. Spark supports Alternating Least Square(ALS) to factor decomposition which that's only one spark supports, because the purpose of spark is generally for big data, ALS doesn't need the algorithm to calculator all the results, so it would save time and computer calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFIf4wdcUzZ7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "(training, test) = review_df1.randomSplit([0.7, 0.3])\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"reviewer_id\", itemCol=\"listing_id\", ratingCol=\"sentiment_compound\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"sentiment_compound\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 house recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each house\n",
    "houseRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvk3epl5cAVa"
   },
   "outputs": [],
   "source": [
    "userRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjBlZdJ-dqF6"
   },
   "outputs": [],
   "source": [
    "houseRecs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on House (Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created the data for this system which is based on the description of houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_most_living = most_living.drop('listing_id').join(df3,\"id\", \"left_outer\").head(50)\n",
    "rec_most_living = spark.createDataFrame(rec_most_living)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built pipeline for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import clustering\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "center = StandardScaler(withMean=True, withStd=False, inputCol='tfidf', outputCol='centered_tfidf')\n",
    "norm = Normalizer(inputCol=\"centered_tfidf\", outputCol=\"norm_tfidf\", p=2.0)\n",
    "kmeans = clustering.KMeans(k=10, featuresCol='norm_tfidf', predictionCol='kmeans_feat')\n",
    "pca = PCA(k=10, inputCol='centered_tfidf', outputCol='scores')\n",
    "rec_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv, idf, center, norm, kmeans, pca])\n",
    "pipeline_model = rec_pipeline.fit(rec_most_living)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a User Defined Function (UDF) that takes as input two column vectors and returns the distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "def l2_dist(c1, c2):    \n",
    "    return float(np.sqrt((c1 - c2).T.dot((c1 - c2))))\n",
    "\n",
    "l2_dist_udf = fn.udf(l2_dist, types.FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommemd(id,num):\n",
    "  print(f'There are {num} houses in Airbnb you could like except {id}')\n",
    "\n",
    "  result = pipeline_model.transform(rec_most_living).\\\n",
    "        where(rec_most_living.id == id).\\\n",
    "        select(fn.col(\"scores\").alias('rec_scores')).\\\n",
    "        join(pipeline_model.transform(rec_most_living)).\\\n",
    "        withColumn('distance', l2_dist_udf('scores', 'rec_scores')).\\\n",
    "        select(\"id\",\"host_name\", \"listing_url\",\"description\", \"distance\").\\\n",
    "        orderBy(fn.asc(\"distance\")).\\\n",
    "        limit(num+1).show()\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommemd(\"5259194\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OeH6aPY8BT4K"
   ],
   "name": "718_project",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "718_project",
  "notebookId": 782275159368530,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
