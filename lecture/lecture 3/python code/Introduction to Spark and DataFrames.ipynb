{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session and Spark Context\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark-intro').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RDDs from Python variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the RDD type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python list containing the first 2 elements of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python list containing all elements in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply functions to each element.  Define such a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_than_10(x):\n",
    "    if x < 10:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show that it is lazy evaluation\n",
    "rdd.filter(less_than_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(less_than_10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(less_than_10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't modify the rdd.  If we convert the rdd to a python list, all original values are unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `square` to apply to each element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x # x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the square function to each element of the rdd using the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 25,\n",
       " 36,\n",
       " 49,\n",
       " 64,\n",
       " 81,\n",
       " 100,\n",
       " 121,\n",
       " 144,\n",
       " 169,\n",
       " 196,\n",
       " 225,\n",
       " 256,\n",
       " 289,\n",
       " 324,\n",
       " 361]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(square).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new function `multiple_of_10` to apply to each element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_of_10(x):\n",
    "    if x % 10 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `multiple_of_10` to each element of the RDD using the map operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(square).filter(multiple_of_10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read only cell\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# get the databricks runtime version\n",
    "db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, grading, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "#\n",
    "# Notes:\n",
    "#   Graders, set the GRADING_RUNTME_ENV environment variable to contain the full path \n",
    "#   to the data file for this assignment.  For example, my grading_env var is set as \n",
    "#   follows on Windows:\n",
    "#   set GRADING_RUNTIME_ENV=c:/Users/Will/Desktop/SU/IST-718/datasets\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # The grading_env variable contains the full path to the \n",
    "    # directory containing the data file.  \n",
    "    grading_env = os.getenv(\"GRADING_RUNTIME_ENV\")\n",
    "    \n",
    "    # if the databricks env var exists\n",
    "    if db_env != None:\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else if the grading environment variable exists\n",
    "    elif grading_env != None:\n",
    "        # build the full path file name assuming a grading env\n",
    "        full_path_name = \"%s/%s\" % (grading_env, data_file_name)\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the rdd from file\n",
    "full_path_file_name = get_training_filename('shakespeare.txt')\n",
    "sotu_rdd = sc.textFile(full_path_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the unique ID for this RDD (within its SparkContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Complete Works of William Shakespeare, by '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the call to `first` actually returns a Python string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sotu_rdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the first 10 elements of the RDD to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ',\n",
       " 'William Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **',\n",
       " '**     Please follow the copyright guidelines in this file.     **']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Check how many times the word `love` appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_love(line):\n",
    "    return line.lower().split().count(\"love\")\n",
    "    # return \"love\" in line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.map(count_love).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1279"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.map(count_love).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_love(line):\n",
    "    # should return True if line has word `love`, and False otherwise\n",
    "    return \"love\" in line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Of his self-love to stop posterity?',\n",
       " '  Calls back the lovely April of her prime,',\n",
       " '  Unthrifty loveliness why dost thou spend,']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.filter(has_love).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first map reduce job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic mapreduce paradigm can be accomplished by using `map` or `flatMap` (if multiple key-value pairs are generated) and `reduceByKey`.  The following RDD contains month, state, and number of orders per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = [\n",
    "['JAN', 'NY', 3.],\n",
    "['JAN', 'PA', 1.],\n",
    "['JAN', 'NJ', 2.],\n",
    "['JAN', 'CT', 4.],\n",
    "['FEB', 'PA', 1.],\n",
    "['FEB', 'NJ', 1.],\n",
    "['FEB', 'NY', 2.],\n",
    "['FEB', 'VT', 1.],\n",
    "['MAR', 'NJ', 2.],\n",
    "['MAR', 'NY', 1.],\n",
    "['MAR', 'VT', 2.],\n",
    "['MAR', 'PA', 3.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallelize method convers the python list to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rdd = sc.parallelize(example_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the number of orders per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1- Generate key-value pairs.  Start out by printing the dataset_rdd.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[17] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dataset_rdd type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the map function to apply to each element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(row):\n",
    "    return [row[0], row[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `map_func` to each element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['JAN', 3.0],\n",
       " ['JAN', 1.0],\n",
       " ['JAN', 2.0],\n",
       " ['JAN', 4.0],\n",
       " ['FEB', 1.0],\n",
       " ['FEB', 1.0],\n",
       " ['FEB', 2.0],\n",
       " ['FEB', 1.0],\n",
       " ['MAR', 2.0],\n",
       " ['MAR', 1.0],\n",
       " ['MAR', 2.0],\n",
       " ['MAR', 3.0]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rdd.map(map_func).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2- Reduce to count the number of orders per month*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the reduce function which will be used by the reduceByKey function to accumulate the totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_func(value1, value2):\n",
    "    return value1 + value2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JAN', 10.0), ('FEB', 5.0), ('MAR', 8.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rdd.map(map_func).reduceByKey(reduce_func).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the average number of orders per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['JAN', 'NY', 3.0],\n",
       " ['JAN', 'PA', 1.0],\n",
       " ['JAN', 'NJ', 2.0],\n",
       " ['JAN', 'CT', 4.0],\n",
       " ['FEB', 'PA', 1.0],\n",
       " ['FEB', 'NJ', 1.0],\n",
       " ['FEB', 'NY', 2.0],\n",
       " ['FEB', 'VT', 1.0],\n",
       " ['MAR', 'NJ', 2.0],\n",
       " ['MAR', 'NY', 1.0],\n",
       " ['MAR', 'VT', 2.0],\n",
       " ['MAR', 'PA', 3.0]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below define a function which will be called in the map function.  The avg_map_func takes a row from the rdd defined above, and returns the value in the first col, and a tuple containing the the value in the 3rd col followd by a 1.  The 1 will be used to count the number of items for the key where the key is the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_map_func(row):\n",
    "    return (row[0], (row[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avg_reduce_func takes value 1 and vlaue 2 as inputs.  Value 1 and value 2 are expected to be the tuples defined in the output from avg_map_func above.  The goal of the function is to add up the floats and the 1's in the tuples.  We are essentially summing up the floats and the 1's associated with each unique key.  Note that the key is not one of the args, the reduceByKey function below will strip the keys out of the data returned by the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_reduce_func(value1, value2):\n",
    "    return ((value1[0] + value2[0], value1[1] + value2[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the map and reduceByKey functions.  The map function returns the month (used as the key for the reduceByKey function), and a tuple containing the 3rd col floating point value followed by a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JAN', (3.0, 1)),\n",
       " ('JAN', (1.0, 1)),\n",
       " ('JAN', (2.0, 1)),\n",
       " ('JAN', (4.0, 1)),\n",
       " ('FEB', (1.0, 1)),\n",
       " ('FEB', (1.0, 1)),\n",
       " ('FEB', (2.0, 1)),\n",
       " ('FEB', (1.0, 1)),\n",
       " ('MAR', (2.0, 1)),\n",
       " ('MAR', (1.0, 1)),\n",
       " ('MAR', (2.0, 1)),\n",
       " ('MAR', (3.0, 1))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rdd.map(avg_map_func).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JAN', (10.0, 4)), ('FEB', (5.0, 4)), ('MAR', (8.0, 4))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rdd.map(avg_map_func).reduceByKey(avg_reduce_func).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we present 2 different ways to compute the final average using `map` and `mapValues` functions to divide the sum of the floats by the sum of the 1's.  The mapValues funtion excludes the keys so there is no need for double indexing.  The sum of the 1's is the number of rows per key so the result is the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mapValues: [('JAN', 2.5), ('FEB', 1.25), ('MAR', 2.0)]\n",
      "Using map: [('JAN', 2.5), ('FEB', 1.25), ('MAR', 2.0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Using mapValues:\", dataset_rdd.map(avg_map_func).reduceByKey(avg_reduce_func).mapValues(lambda x: x[0]/x[1]).collect())\n",
    "print(\"Using map:\", dataset_rdd.map(avg_map_func).reduceByKey(avg_reduce_func).map(lambda x: (x[0], x[1][0]/x[1][1])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the frequency of words appearing in the Shakespeare sonets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 lines of sotu_rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ',\n",
       " 'William Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **',\n",
       " '**     Please follow the copyright guidelines in this file.     **']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some utility functions to use in flatMap and reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 320),\n",
       " ('gutenberg', 250),\n",
       " ('ebook', 13),\n",
       " ('of', 18126),\n",
       " ('shakespeare', 270),\n",
       " ('this', 5930),\n",
       " ('is', 9168),\n",
       " ('use', 509),\n",
       " ('anyone', 5),\n",
       " ('anywhere', 4)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(corpus):\n",
    "    return [(word.lower(), 1) for word in corpus.split()]\n",
    "\n",
    "def sum_words(first, second):\n",
    "    return first + second\n",
    "\n",
    "sotu_rdd.flatMap(count_words).reduceByKey(sum_words).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break up the flatMap and reduceByKey operations. The flatMap operation takes a single element (in this case a list of words), and returns 0 or more output items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('project', 1),\n",
       " ('gutenberg', 1),\n",
       " ('ebook', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('complete', 1),\n",
       " ('works', 1),\n",
       " ('of', 1),\n",
       " ('william', 1),\n",
       " ('shakespeare,', 1),\n",
       " ('by', 1),\n",
       " ('william', 1),\n",
       " ('shakespeare', 1),\n",
       " ('this', 1),\n",
       " ('ebook', 1),\n",
       " ('is', 1),\n",
       " ('for', 1),\n",
       " ('the', 1),\n",
       " ('use', 1),\n",
       " ('of', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('at', 1),\n",
       " ('no', 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.flatMap(count_words).take(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we add the reduceByKey function onto the flatMap function, the reduceByKey function groups common words by key, and adds up all the ones associated with each word / key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 320),\n",
       " ('gutenberg', 250),\n",
       " ('ebook', 13),\n",
       " ('of', 18126),\n",
       " ('shakespeare', 270),\n",
       " ('this', 5930),\n",
       " ('is', 9168),\n",
       " ('use', 509),\n",
       " ('anyone', 5),\n",
       " ('anywhere', 4)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sotu_rdd.flatMap(count_words).reduceByKey(sum_words).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create `DataFrames` programatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python list of spark row objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [Row(state='NY', month='JAN', orders=3),\n",
    "            Row(state='NJ', month='JAN', orders=4),\n",
    "            Row(state='NY', month='FEB', orders=5),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data:\n",
      " [Row(month='JAN', orders=3, state='NY'), Row(month='JAN', orders=4, state='NJ'), Row(month='FEB', orders=5, state='NY')]\n",
      "type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"raw_data:\\n\", raw_data)\n",
    "print(\"type:\", type(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spark data frame from the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the spark dataframe and it's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df: DataFrame[month: string, orders: bigint, state: string]\n",
      "type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"spark_df:\", spark_df)\n",
    "print(\"type:\", type(spark_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema.  The dataframe schema defines the column names and types (and other things)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- orders: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  JAN|     3|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "|  FEB|     5|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another python list containing new raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2 = [Row(state='NY', month='MAR', orders=10),\n",
    "             Row(state='NJ', month='MAR', orders=3),\n",
    "             Row(state='NY', month='APR', orders=1),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new spark dataframe from the new raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df2 = spark.createDataFrame(raw_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the 2 data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_df:\n",
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  JAN|     3|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "|  FEB|     5|   NY|\n",
      "+-----+------+-----+\n",
      "\n",
      "spark_df2:\n",
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  MAR|    10|   NY|\n",
      "|  MAR|     3|   NJ|\n",
      "|  APR|     1|   NY|\n",
      "+-----+------+-----+\n",
      "\n",
      "union:\n",
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  JAN|     3|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "|  FEB|     5|   NY|\n",
      "|  MAR|    10|   NY|\n",
      "|  MAR|     3|   NJ|\n",
      "|  APR|     1|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"spark_df:\")\n",
    "spark_df.show()\n",
    "print(\"spark_df2:\") \n",
    "spark_df2.show()\n",
    "print(\"union:\")\n",
    "all_data_df = spark_df.union(spark_df2)\n",
    "all_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or display the merged data frame using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>orders</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>JAN</td>\n",
       "      <td>3</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>4</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>FEB</td>\n",
       "      <td>5</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MAR</td>\n",
       "      <td>10</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MAR</td>\n",
       "      <td>3</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>APR</td>\n",
       "      <td>1</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  month  orders state\n",
       "0   JAN       3    NY\n",
       "1   JAN       4    NJ\n",
       "2   FEB       5    NY\n",
       "3   MAR      10    NY\n",
       "4   MAR       3    NJ\n",
       "5   APR       1    NY"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you limit first\n",
    "all_data_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`display` produces a nice formatted tabular output in databricks.  `display` does work outside of databricks but the output is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, orders: bigint, state: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(all_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'month'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'month'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(month + 1)'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df['month'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to print out the data in the above functions because columns do not have methods to show the data.  Another way to see data in the columns is using the `select` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|  JAN|\n",
      "|  JAN|\n",
      "|  FEB|\n",
      "|  MAR|\n",
      "|  MAR|\n",
      "|  APR|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.select('month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_month_jan = (all_data_df['month'] == \"JAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(month = JAN)'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_month_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, orders: bigint, state: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.where(condition_month_jan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, orders: bigint, state: string]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df[condition_month_jan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  JAN|     3|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df[condition_month_jan].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditions are symbolic objects.  Create a column selection variable named logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = (all_data_df['month']  == 'MAR') & (all_data_df['orders'] > 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the variable named logic to select data in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  MAR|    10|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df[logic].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  JAN|     3|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "|  FEB|     5|   NY|\n",
      "|  MAR|    10|   NY|\n",
      "|  MAR|     3|   NJ|\n",
      "|  APR|     1|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(orders + 1)'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df['orders'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- orders: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- order_plus_1: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.withColumn('order_plus_1', all_data_df['orders'] + 1).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+------------+\n",
      "|month|orders|state|order_plus_1|\n",
      "+-----+------+-----+------------+\n",
      "|  JAN|     3|   NY|           4|\n",
      "|  JAN|     4|   NJ|           5|\n",
      "|  FEB|     5|   NY|           6|\n",
      "|  MAR|    10|   NY|          11|\n",
      "|  MAR|     3|   NJ|           4|\n",
      "|  APR|     1|   NY|           2|\n",
      "+-----+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.withColumn('order_plus_1', all_data_df['orders'] + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform some basic grouping operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x150f716b948>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.groupBy('month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, count: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.groupBy('month').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|  APR|    1|\n",
      "|  FEB|    1|\n",
      "|  JAN|    2|\n",
      "|  MAR|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.groupBy('month').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can order by a certain column or group of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  APR|     1|   NY|\n",
      "|  JAN|     3|   NY|\n",
      "|  MAR|     3|   NJ|\n",
      "|  JAN|     4|   NJ|\n",
      "|  FEB|     5|   NY|\n",
      "|  MAR|    10|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.orderBy('orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  MAR|    10|   NY|\n",
      "|  FEB|     5|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "|  MAR|     3|   NJ|\n",
      "|  JAN|     3|   NY|\n",
      "|  APR|     1|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data_df.orderBy('orders', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can register as tables and perform SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df.registerTempTable('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count(1): bigint]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select count(*) from orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       6|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from orders').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFrame` object can read from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-7783f311873a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# this is for databricks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/FileStore/tables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# this is for databricks\n",
    "display(dbutils.fs.ls(\"/FileStore/tables\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path_spotify_name = get_training_filename('spotify_songs.csv')\n",
    "spotify_df = spark.read.csv(full_path_spotify_name, header=True, inferSchema=True, mode=\"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spotify_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data bricks changed the name of the json we uploaded!  The original name was sample-S2-records.json and the file name on databricks is sample_S2_records-3dd18.json.  If you look carefully, you will notice that in addition to adding '-3dd18' to the file name, databricks also changed some dash characters to underscores (sample-S2-records -> sample_S2_records).  The cell below shows hot to get help on databricks filesystem utils.  The cell after that uses the databricks filesystem utils to do a directory listing if we are running on databcicks.  See [databricks_utils_help](https://docs.databricks.com/dev-tools/databricks-utils.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get help for the databricks filesystem ls command\n",
    "if db_env != None:\n",
    "    dbutils.fs.help(\"ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell finds the name of the S2 records json file \n",
    "\n",
    "# if we are running in the databricks environment\n",
    "if db_env != None:\n",
    "  # get a list of dbfs file objects\n",
    "  files = dbutils.fs.ls(\"/FileStore/tables\")\n",
    "  \n",
    "  # for each file object in the list of file objects\n",
    "  for file in files:\n",
    "    # if the string 'json' is in the file name\n",
    "    if \"json\" in file.name and \"S2\" in file.name:\n",
    "      # print the file name\n",
    "      print(file.name)\n",
    "      \n",
    "      # save the file name\n",
    "      renamed_s2_file_name = file.name\n",
    "      \n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are running on databricks\n",
    "if db_env != None:\n",
    "  # read the semantic scholar json file using the specific json file name found above \n",
    "  semantic_scholar_full_path_name = get_training_filename(renamed_s2_file_name)\n",
    "  semantic_scholar = spark.read.json(semantic_scholar_full_path_name)\n",
    "else:\n",
    "  semantic_scholar_full_path_name = get_training_filename(\"sample-S2-records.json\")\n",
    "  semantic_scholar = spark.read.json(semantic_scholar_full_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- doiUrl: string (nullable = true)\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- journalPages: string (nullable = true)\n",
      " |-- journalVolume: string (nullable = true)\n",
      " |-- outCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- paperAbstract: string (nullable = true)\n",
      " |-- pdfUrls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- s2PdfUrl: string (nullable = true)\n",
      " |-- s2Url: string (nullable = true)\n",
      " |-- sources: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# complex schema\n",
    "semantic_scholar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(authors=[Row(ids=['6323488'], name='Jose Alejandro Rauh-Hain'), Row(ids=['40439126'], name='Noah Rodriguez'), Row(ids=['5695062'], name='Whitfield B. Growdon'), Row(ids=['47348020'], name='Anne Kathryn Goodman'), Row(ids=['21470081'], name='David M. Boruta Ii'), Row(ids=['8523393'], name='Neil S Horowitz'), Row(ids=['16734596'], name='Mph Marcela  G.  del  Carmen  MD'), Row(ids=['1739284'], name='John Schorge')], doi='10.1245/s10434-011-2100-x', doiUrl='https://doi.org/10.1245/s10434-011-2100-x', entities=['Epithelial ovarian cancer', 'Excision', 'Extraction', 'Hospital admission', 'Malignant neoplasm of ovary', 'Morbidity - disease rate', 'Neoadjuvant Therapy', 'Neoplasms', 'Overall Survival', 'Patients', 'Postoperative Complications', 'Residual Tumor', 'SLC13A5 gene', 'Stage IV Ovarian Carcinoma', 'Tumor Debulking', 'intensive care unit', 'ovarian neoplasm', \"stage IV childhood Hodgkin's lymphoma\"], id='4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e', inCitations=['505715691a8360f67f1cfcb4f2a45c8652a01612', '450a0ef76fd457deb30a99c6ca22ae30263273fc', 'edf1163e8dfcf0575842d03dfc143e6457d0ff5c', '2282d6014362447b8d9b983bf43ca71d67e48c40', 'b9d911352437e72e621f80f889709fa849779138', 'd4aff3783a894062b9f43d657206f573f483fbfd', 'cbb86bd24472903eb8d54e6aa332fe7f805e307a', 'ede0d95ef1265dbc27021eb9ae9baf2db3bc0dc8', '0bfe29a5f6e572098eb0f5c31ab582bf04328c69', '80d103a7b039c46ac3cf3239873adc053ea6afc8', '7865324b0684bf8a07dd4a6cf2e0a5b1fa64fc1f', '655a5ceb46d974b935695c5f0074a7f9b643958a', '2f092623673c875e6accd5a978a10507be306500', 'f51536c9fd2eeacc88abf84baf7b2f05282db841', 'c7ed54a5eb89086e0fdf7622fbe88e96890823b7', '0cfcae93bf4a955ee1f6b916adf8aebdf8db9aa0', '15344e7d627d73551276d5bdf76039ef74461d26', 'c67082501169a383c972aea0ab9d7f1c72ee43f0', '0a60c97d4eceabfb08e8328da67b176bffe36a67', '065225e5382d8f998481857b45090074494db957', '294f252ac5e0c05c12943bac11ea7a548dbb2620', '64df8fea136c83217d239d4bee993750ba15f75e', 'e005949227ccfe033f578eb11a9c3b546adda2fa', '1c18b95b762a9d9d53852bf20c5c5382dadfbd96', '81212e90971f4499b2c293f293ab47620460e764', '786f4f5b397a0d8aedbe348cc1d3365a3a63b121', '9324bd563c8fdaa0d8d49b6120519ef346a90c9a'], journalName='Annals of Surgical Oncology', journalPages='959-965', journalVolume='19', outCitations=['166ac4b6f694c68dafdc912ca0c336b4c444fd9e', 'f0223f8d1920009d1afccffe2d4129f2211711cf', 'aaee9e127e63a4ee8baae2bc8a960f4a42afce78', '03029e4427cfe66c3da6257979dc2d5b6eb3a0e4', 'd14434966fd87e94c97ed88938ea3dd5282d7652', 'd6fa05d67f9a6fc3256d05c81a4c55c472e78b0c', 'e9b9d08937fd2e603d5f8106ccaf81589dfa9cdf', '943c32265fa79b20b925f6cc450db19b21c47bc0', '07407593b49bc95f8d0c3c3ab912d78564db3302', 'fedaf6c7ea8b58e501b667603b8a23f1756ce375', 'ce84de64d8388258243244bbb4aef290e6022d30', '399e7acd3fb9fa3f7bcf75e378e6176f7f2e83d7', 'a81a07f476fa375b0ef97565c615befd68305807', 'a2d2786451f8999b2cd576b01051c35412abfeab', '5808049452b7ba17c83eb59f8eac680a43fb356d', 'cc7e57dc58ce000a1e208a9a05036411ce81e44b', 'f9114074480dbfcd5383edef4342763850928794', '74f808b2130eec1d7a310c9dded88252ce5e95f9', 'd1c5962330475efac873a0beca438612457a9a7f', '3047fddd63530f7beb6a3949df8ef2fa74ea8076', 'f846c842c76848e7a24bc3f3a0c98f9cf51ef43e', 'eec37a9783db2fce758e5257366f21f5bbb6ee1c', '5b01531c37b4965b6094a7c4ceb9da01d0d81225', '02235d796e51dd058b3454f85ed94b7e9ef3a029', '992ba166b4f49e8818c83e55ba5e9abec6c00f15', '08b568c4cb2b021b64f2c8ace56a077b07402eac', '277c53725dd4733981f1545ebd1faaad7dee8930', '896658482670249ab4c2db9fc3be424178ffc828', 'b653b7a6bd4680ff536d91a7a174088cd2f43976'], paperAbstract='Primary debulking surgery (PDS) has historically been the standard treatment for advanced ovarian cancer. Recent data appear to support a paradigm shift toward neoadjuvant chemotherapy with interval debulking surgery (NACT-IDS). We hypothesized that stage IV ovarian cancer patients would likely benefit from NACT-IDS by achieving similar outcomes with less morbidity. Patients with stage IV epithelial ovarian cancer who underwent primary treatment between January 1, 1995 and December 31, 2007, were identified. Data were retrospectively extracted. Each patient record was evaluated to subclassify stage IV disease according to the sites of tumor dissemination at the time of diagnosis. The KaplanMeier method was used to compare overall survival (OS) data. A total of 242 newly diagnosed stage IV epithelial ovarian cancer patients were included in the final analysis; 176 women (73%) underwent PDS, 45 (18%) NACT-IDS, and 21 (9%) chemotherapy only. The frequency of achieving complete resection to no residual disease was significantly higher in patients with NACT-IDS versus PDS (27% vs. 7.5%; P\\xa0<\\xa00.001). When compared to women treated with NACT-IDS, women with PDS had longer admissions (12 vs. 8\\xa0days; P\\xa0=\\xa00.01), more frequent intensive care unit admissions (12% vs. 0%; P\\xa0=\\xa00.01), and a trend toward a higher rate of postoperative complications (27% vs. 15%; P\\xa0=\\xa00.08). The patients who received only chemotherapy had a median OS of 23\\xa0months, compared to 33\\xa0months in the NACT-IDS group and 29\\xa0months in the PDS group (P\\xa0=\\xa00.1). NACT-IDS for stage IV ovarian cancer resulted in higher rates of complete resection to no residual disease, less morbidity, and equivalent OS compared to PDS.', pdfUrls=[], pmid='21994038v1', s2PdfUrl='', s2Url='https://semanticscholar.org/paper/4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e', sources=['Medline'], title='Primary Debulking Surgery Versus Neoadjuvant Chemotherapy in Stage IV Ovarian Cancer', venue='Annals of Surgical Oncology', year=2011)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_scholar.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below cells implement map reduce jobs from the lecture slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "min: (1, 0)\n",
      "type(rdd.map(map_func).reduce(reduce_min)) <class 'tuple'>\n",
      "max: (1, 9)\n",
      "sum: 45\n"
     ]
    }
   ],
   "source": [
    "def map_func(el):\n",
    "    return (1, el)\n",
    "\n",
    "def reduce_min(el1, el2):\n",
    "    return min(el1, el2)\n",
    "\n",
    "def reduce_max(el1, el2):\n",
    "    return max(el1, el2)\n",
    "\n",
    "def reduce_sum(el1, el2):\n",
    "    return el1 + el2\n",
    "\n",
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "print(\"rdd:\", rdd.collect())\n",
    "print(\"min:\", rdd.map(map_func).reduce(reduce_min))\n",
    "print(\"type(rdd.map(map_func).reduce(reduce_min))\", type(rdd.map(map_func).reduce(reduce_min)))\n",
    "print(\"max:\", rdd.map(map_func).reduce(reduce_max))\n",
    "print(\"sum:\", rdd.reduce(reduce_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map only:\n",
      " [[('foo', 1), ('foo1', 1), ('foo2', 1)], [('bar', 1), ('bar1', 1), ('bar2', 1)], [('foobar', 1), ('foobar1', 1), ('foobar2', 1)]]\n",
      "map and reduce:\n",
      " [('foo', 1), ('foo1', 1), ('foo2', 1), ('bar', 1), ('bar1', 1), ('bar2', 1), ('foobar', 1), ('foobar1', 1), ('foobar2', 1)]\n"
     ]
    }
   ],
   "source": [
    "data = [\"foo foo1 foo2\", \"bar bar1 bar2\", \"foobar foobar1 foobar2\"]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "def f(e):\n",
    "    results = []\n",
    "    for word in e.split():\n",
    "        results.append((word, 1))\n",
    "    return results\n",
    "\n",
    "print(\"map only:\\n\", rdd.map(f).collect())\n",
    "print(\"map and reduce:\\n\", rdd.map(f).reduce(lambda v1, v2: v1 + v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Introduction to Spark and DataFrames",
  "notebookId": 2027093944953402
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
